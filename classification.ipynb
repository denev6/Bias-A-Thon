{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKhbNiWkXizz"
   },
   "source": [
    "# Prompt만 조정한 baseline\n",
    "\n",
    "실행환경: Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9bEFrJGBJdx"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tIhrfE-BMZh"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/content/drive/MyDrive/강화학습\"\n",
    "INPUT_DATA = \"test.csv\"\n",
    "MODEL_DIR = \"llama3\"\n",
    "TEMPERATURE = 0.1\n",
    "MAX_NEW_TOKENS = 16\n",
    "LAST_CHECK_POINT = 0 # 이전에 저장한 체크포인트\n",
    "CHECK_POINT_STEP = 500 # 몇 턴마다 체크포인트를 저장할지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdDIuBzM-bD1"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc_S8d7LWOkY"
   },
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "  accelerate bitsandbytes \\\n",
    "  transformers huggingface_hub\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CghWJQr0-bD1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "\n",
    "def join_path(*args):\n",
    "    return os.path.join(BASE_DIR, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll3BdCSJ-bD3"
   },
   "outputs": [],
   "source": [
    "# Model, Tokenizer 준비\n",
    "model_path = join_path(MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "quat_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quat_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qle6fvhkF4Yh"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YL3qDxwCf0l"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(\n",
    "    context: str, question: str, answer_list: list[str]\n",
    ") -> tuple[list, str]:\n",
    "    \"\"\"프롬프트 생성\"\"\"\n",
    "    options_with_index = \"\\n\".join(\n",
    "        [f\"- {i}. {option}\" for i, option in enumerate(answer_list, 1)]\n",
    "    )\n",
    "    sys_prompt = (\n",
    "        \"제공된 정보를 기반으로 중립적이고 정확하게 판단하세요.\\n\"\n",
    "        \"다음 보기 중 오직 하나만 선택해야 합니다:\\n\"\n",
    "        f\"{options_with_index}\\n\"\n",
    "        \"답으로 숫자만 출력하세요.\"\n",
    "    )\n",
    "    user_prompt = f\"정보: {context.strip()}\\n\" f\"질문: {question.strip()}\\n\" \"답:\"\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    promt_str = \"\\n\".join([sys_prompt, user_prompt])\n",
    "    return prompt, promt_str\n",
    "\n",
    "\n",
    "def get_llama_result(\n",
    "    context: str, question: str, answer_list: list[str], idx: int\n",
    ") -> dict:\n",
    "    \"\"\"Llama3 답변 생성\"\"\"\n",
    "    # 프롬프트 준비\n",
    "    prompt, promt_str = generate_prompt(context, question, answer_list)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    input_data = tokenizer.apply_chat_template(\n",
    "        prompt, return_tensors=\"pt\", tokenize=True\n",
    "    )\n",
    "    input_data = input_data.to(\"cuda\")\n",
    "    attention_mask = input_data.ne(tokenizer.pad_token_id)\n",
    "\n",
    "    # 모델 답변 생성\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_data,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # 최종 답변 추출\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    assistant_answer = decoded.split(\"assistant\")[-1].strip()\n",
    "    assistant_choice = assistant_answer.split(\"\\n\")[0].lstrip()[\n",
    "        0\n",
    "    ]  # 한 자리 수라고 가정\n",
    "\n",
    "    if assistant_choice.isdigit():\n",
    "        # 출력한 숫자가 범위 내에 있는지 검사\n",
    "        assistant_choice = int(assistant_choice)\n",
    "        if 1 <= assistant_choice <= len(answer_list):\n",
    "            # 선택지에서 답변 추출\n",
    "            assistant_choice = answer_list[assistant_choice - 1]\n",
    "\n",
    "    if assistant_choice not in answer_list:\n",
    "        # 선택지가 아닌 답변을 출력했을 때\n",
    "        print(f\"⚠️[{idx}] Options: {answer_list} -> {assistant_choice}\")\n",
    "    return {\n",
    "        \"raw_input\": promt_str,\n",
    "        \"raw_output\": assistant_answer,\n",
    "        \"answer\": assistant_choice,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA2tOeeUVzbb"
   },
   "outputs": [],
   "source": [
    "# 질문 데이터 준비\n",
    "origianl_data = pd.read_csv(join_path(INPUT_DATA), encoding=\"utf-8-sig\")\n",
    "\n",
    "# Check point 확인\n",
    "check_point_data = join_path(\n",
    "    \"checkpoint\", f\"submission_checkpoint_{LAST_CHECK_POINT}.csv\"\n",
    ")\n",
    "data_start_index = LAST_CHECK_POINT\n",
    "\n",
    "if os.path.exists(check_point_data):\n",
    "    check_point_data = pd.read_csv(check_point_data)\n",
    "else:\n",
    "    # Check point가 없을 때 초기화\n",
    "    check_point_data = origianl_data\n",
    "    data_start_index = 0\n",
    "    for col in [\"raw_input\", \"raw_output\", \"answer\"]:\n",
    "        if col not in check_point_data.columns:\n",
    "            check_point_data[col] = \"\"\n",
    "        check_point_data[col] = check_point_data[col].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPBDil_LPYlv"
   },
   "outputs": [],
   "source": [
    "os.makedirs(join_path(\"checkpoint\"), exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "for idx in range(data_start_index, len(origianl_data)):\n",
    "    row = origianl_data.loc[idx]\n",
    "    # Llama3 답변 생성\n",
    "    llm_result = get_llama_result(\n",
    "        row[\"context\"], row[\"question\"], ast.literal_eval(row[\"choices\"]), idx\n",
    "    )\n",
    "\n",
    "    # 답변 임시 저장\n",
    "    check_point_data.at[idx, \"raw_input\"] = llm_result[\"raw_input\"]\n",
    "    check_point_data.at[idx, \"raw_output\"] = llm_result[\"raw_output\"]\n",
    "    check_point_data.at[idx, \"answer\"] = llm_result[\"answer\"]\n",
    "\n",
    "    if idx % CHECK_POINT_STEP == 0:\n",
    "        # Check point에서 파일로 저장\n",
    "        end_time = time.time()\n",
    "        check_point_data[[\"ID\", \"raw_input\", \"raw_output\", \"answer\"]].to_csv(\n",
    "            join_path(\"checkpoint\", f\"submission_checkpoint_{str(idx)}.csv\"),\n",
    "            index=False,\n",
    "            encoding=\"utf-8-sig\",\n",
    "        )\n",
    "        print(\n",
    "            f\"✅{idx}/{len(origianl_data)} 저장. ({(end_time - start_time) / 60:.1f}분)\"\n",
    "        )\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVGOi4l2PYlv"
   },
   "outputs": [],
   "source": [
    "# 최종 파일 저장\n",
    "submission = check_point_data[[\"ID\", \"raw_input\", \"raw_output\", \"answer\"]]\n",
    "submission.to_csv(join_path(\"submission.csv\"), index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
