{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline in Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdDIuBzM-bD1"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc_S8d7LWOkY"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUJ0-3rlP8v8"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CghWJQr0-bD1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0610tVM8QJ2Q"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "\n",
    "def join_path(*args):\n",
    "    return os.path.join(\"/content/drive/MyDrive/강화학습\", *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueRnHV3E-bD2"
   },
   "outputs": [],
   "source": [
    "INPUT_DATA_PATH = join_path(\"test.csv\")\n",
    "OUTPUT_DATA_PATH = join_path(\"submission.csv\")\n",
    "MODEL_PATH = join_path(\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcfK1AVnPYlu"
   },
   "source": [
    "## Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll3BdCSJ-bD3"
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # 본 대회는 반드시 Llama-3.1-8B-Instruct 모델만 사용해야 합니다.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,  # 원본 모델은 Colab(VRAM 16GB)에서 돌아가지 않습니다... GPU 자원 있으면 알려주세요.\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "# 4bit + fp16으로 돌리니까 VRAM 7.2GB 정도 먹네요\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9CWqvupRh7l"
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(MODEL_PATH)\n",
    "model.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA2tOeeUVzbb"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(INPUT_DATA_PATH, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p_hRgwdPYlu"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z523_se8-bD8"
   },
   "outputs": [],
   "source": [
    "def make_prompt(context, question, choices):\n",
    "    choices = ast.literal_eval(choices)\n",
    "\n",
    "    # 프롬프트를 수정하여 모델에 전달할 수 있습니다.\n",
    "    # 예시 프롬프트\n",
    "    return f\"\"\"질문에 대해서 다음 선택지 중 반드시 하나만 답하시오. 다른 선택지는 고려하지 마시오.\n",
    "\n",
    "            질문 : {context} {question}\n",
    "            선택지: {choices[0]} ,{choices[1]} ,{choices[2]}\n",
    "\n",
    "            답변:\"\"\"\n",
    "\n",
    "\n",
    "def extract_answer(text):\n",
    "    # 대회 측에서 제시한 예시이고 당연히 다른 방법 써도 됩니다.\n",
    "    raw_answer = text.split(\"답변:\")[-1].strip()  # 프롬프트를 제외한 답변만 추출\n",
    "    result = re.search(r\"답변:\\s*([^\\n\\r:]+)\", text)  # 정규 표현식으로 답변 추출\n",
    "    answer = result.group(1).strip() if result else None\n",
    "    return raw_answer, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YL3qDxwCf0l"
   },
   "outputs": [],
   "source": [
    "def predict_answer(context, question, choices, max_new_tokens=16):\n",
    "    prompt = make_prompt(context, question, choices)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    result = tokenizer.decode(output[-1], skip_special_tokens=True)\n",
    "    raw_answer, answer = extract_answer(result)\n",
    "\n",
    "    return pd.Series({\"raw_input\": prompt, \"raw_output\": raw_answer, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPBDil_LPYlv"
   },
   "outputs": [],
   "source": [
    "check_point_dir = join_path(\"checkpoint\")\n",
    "os.makedirs(check_point_dir, exist_ok=True)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    row = data.loc[i]\n",
    "    result = predict_answer(row[\"context\"], row[\"question\"], row[\"choices\"])\n",
    "\n",
    "    data.at[i, \"raw_input\"] = result[\"raw_input\"]\n",
    "    data.at[i, \"raw_output\"] = result[\"raw_output\"]\n",
    "    data.at[i, \"answer\"] = result[\"answer\"]\n",
    "\n",
    "    # 5000개마다 중간 저장\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"✅ Processing {i}/{len(data)} — 중간 저장 중...\")\n",
    "        data[[\"ID\", \"raw_input\", \"raw_output\", \"answer\"]].to_csv(\n",
    "            join_path(\"checkpoint\", f\"submission_checkpoint_{str(i)}.csv\"),\n",
    "            index=False,\n",
    "            encoding=\"utf-8-sig\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlzmNSmYPYlv"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVGOi4l2PYlv"
   },
   "outputs": [],
   "source": [
    "submission = data[[\"ID\", \"raw_input\", \"raw_output\", \"answer\"]]\n",
    "submission.to_csv(OUTPUT_DATA_PATH, index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
