{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKhbNiWkXizz"
   },
   "source": [
    "# Self-debiasing\n",
    "\n",
    "`Self-debiasing`ê³¼ `Human-Persona`ë¥¼ ì ìš©í•œ ì½”ë“œ.\n",
    "\n",
    "`masking` ë° `few-shot`ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.\n",
    "\n",
    "ì‹¤í–‰í™˜ê²½: Colab T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9bEFrJGBJdx"
   },
   "source": [
    "## ì‚¬ìš©ì ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tIhrfE-BMZh"
   },
   "outputs": [],
   "source": [
    "# ğŸ”¥í•˜ì´í¼íŒŒë¦¬ë¯¸í„° ì„¤ì •\n",
    "BASE_DIR = \"/content/drive/MyDrive/ê°•í™”í•™ìŠµ\"\n",
    "CHECKPOINT_DIR = \"checkpoint\"\n",
    "INPUT_DATA = \"test.csv\"\n",
    "MODEL_DIR = \"llama3\"\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE = 0.6  # ì»¤ì§ˆìˆ˜ë¡ ë‹µë³€ì˜ ììœ ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.\n",
    "MAX_NEW_TOKENS = 512  # ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹µë³€ì´ ëê¹Œì§€ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "TOKENIZER_MAX_LENGTH = 1024  # ë„ˆë¬´ ì§§ìœ¼ë©´ í”„ë¡¬í”„íŠ¸ê°€ ì§¤ë¦½ë‹ˆë‹¤.\n",
    "CHECK_POINT_STEP = 100  # ëª‡ í„´ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í• ì§€\n",
    "NUM_WORKERS = 2  # Colab ë¬´ë£Œ í‹°ì–´ë¥¼ ì‚¬ìš©í•  ê²½ìš° 2ë³´ë‹¤ í‚¤ìš°ì§€ ë§ˆì„¸ìš”.\n",
    "BATCH_SIZE = 32  # Cuda-Out-of-Memoryê°€ ë‚˜ë©´ 2ë°°ì”© ì¤„ì´ì„¸ìš”.\n",
    "RANDOM_SEED = 33\n",
    "\n",
    "# (int) ì´ì „ì— ì €ì¥í•œ ì²´í¬í¬ì¸íŠ¸\n",
    "LAST_FLAG_CHECK_POINT = 0\n",
    "LAST_PROMPT_CHECK_POINT = 0\n",
    "LAST_SUBMISSION_CHECK_POINT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMVxfk3yT6Cr"
   },
   "outputs": [],
   "source": [
    "def generate_bias_determination(prompt) -> str:\n",
    "    return (\n",
    "        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"[í”„ë¡¬í”„íŠ¸]: {prompt}\\n\\n\"\n",
    "        \"ë¨¼ì € í”„ë¡¬í”„íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ë‹¤ìŒ, \"\n",
    "        \"ê·¸ ë¬¸ì¥ë“¤ì´ íŒë‹¨ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ì¸ì§€ì  í¸í–¥ì„ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ ë¶„ì„í•˜ì„¸ìš”.\\n\"\n",
    "        \"í¸í–¥ì´ ìˆëŠ”ì§€ í•œ ë‹¨ì–´ë¡œ ë‹µí•˜ì„¸ìš”. (ì˜ˆ, ì•„ë‹ˆì˜¤): <|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def has_yes(response):\n",
    "    for yes in [\"ë„¤\", \"ì˜ˆ\"]:\n",
    "        if yes in response:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_bias_analysis(prompt) -> str:\n",
    "    return (\n",
    "        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        \"ë‹¤ìŒì€ ì¸ì§€ì  í¸í–¥ì„ í¬í•¨í•˜ê³  ìˆì„ ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.\\n\"\n",
    "        \"ê° ë¬¸ì¥ì— ì–´ë–¤ ì¸ì§€ì  í¸í–¥ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ë¶„ì„í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”.<|eot_id|>\\n\"\n",
    "        f\"[í”„ë¡¬í”„íŠ¸]: {prompt}\\n\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_cognitive_debiasing() -> str:\n",
    "    return (\n",
    "        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        \"í¸í–¥ íŒë‹¨ì— ë”°ë¼ ì‚¬ëŒì´ í¸í–¥ë˜ì§€ ì•Šë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ë˜, \"\n",
    "        \"ì›ë˜ ì§ˆë¬¸ì˜ ëª©ì ì€ ìœ ì§€í•˜ì„¸ìš”.\\n\"\n",
    "        \"ì„¤ëª… ì—†ì´ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_question(prompt: str, choices: list) -> str:\n",
    "    choices_with_idx = \"\\n\".join(\n",
    "        [f\"{i}. {choice}\" for i, choice in enumerate(choices, 1)]\n",
    "    )\n",
    "    return (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
    "        \"ëŠë¦¬ê³  ì‹ ì¤‘í•˜ê²Œ ë‹µí•˜ëŠ” ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ì„¸ìš”.\\n\"\n",
    "        \"ê·¸ë“¤ì˜ ë‹µì€ ì„±ì˜ ìˆê³  ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\"\n",
    "        \"ì´ ìì•„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì•„ë˜ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.<|eot_id|>\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\"\n",
    "        f\"[í”„ë¡¬í”„íŠ¸]: {prompt.strip()}\\n\"\n",
    "        \"[ì„ íƒì§€]:\\n\"\n",
    "        f\"{choices_with_idx}\\n\\n\"\n",
    "        \"ìµœì¢… ë‹µë³€ì€ ì„¤ëª… ì—†ì´ 1, 2, 3 ì¤‘ í•˜ë‚˜ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.\\n\"\n",
    "        \"ìµœì¢… ë‹µë³€:<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def trim_assistant_response(response):\n",
    "    response = response.rsplit(\"assistant\", 1)[-1]\n",
    "    return response.strip(\"\\n\").strip()\n",
    "\n",
    "\n",
    "def extract_last_choice(raw_answer, choices):\n",
    "    # ğŸ”¥ìµœì¢… ë‹µë³€ì—ì„œ ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”.\n",
    "    first_digit = next(\n",
    "        (char for char in raw_answer if char.isdigit()), None\n",
    "    )  # ì²˜ìŒ ë‚˜ì˜¤ëŠ” ìˆ«ìë§Œ ì¶”ì¶œ\n",
    "    if first_digit.isdigit():\n",
    "        # 1 ~ 3ìœ¼ë¡œ ë‹µí•  ê²½ìš°, ì •ë‹µì§€ì—ì„œ ë‹µë³€ ì„ íƒ\n",
    "        last_choice_idx = int(first_digit)\n",
    "        if 1 <= last_choice_idx <= 3:\n",
    "            last_choice = choices[last_choice_idx - 1]\n",
    "            return last_choice\n",
    "\n",
    "    # ì´ìƒí•œ ë‹µì´ ë‚˜ì˜¬ ê²½ìš°, ê·¸ëŒ€ë¡œ ë±‰ê¸°\n",
    "    raw_answer = raw_answer.strip().replace(\"\\n\", \"\")\n",
    "    print(f\"âš ï¸ë‹µë³€ì´ ì´ìƒí•´ìš”. [{raw_answer}]\")\n",
    "    return raw_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdDIuBzM-bD1"
   },
   "source": [
    "## ëª¨ë¸ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq2sRkT_VCfY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPUë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc_S8d7LWOkY"
   },
   "outputs": [],
   "source": [
    "!pip install -qq accelerate bitsandbytes transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CghWJQr0-bD1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import gc\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "\n",
    "def join_path(*args):\n",
    "    return os.path.join(BASE_DIR, *args)\n",
    "\n",
    "\n",
    "def collect_garbage():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "def save_csv(df, path: str, cols: list[str]):\n",
    "    df[cols].to_csv(\n",
    "        path,\n",
    "        index=False,\n",
    "        encoding=\"utf-8-sig\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll3BdCSJ-bD3"
   },
   "outputs": [],
   "source": [
    "# Model, Tokenizer ì¤€ë¹„\n",
    "# model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_path = join_path(MODEL_DIR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "quat_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quat_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvLCeeL6hdO8"
   },
   "outputs": [],
   "source": [
    "# CUDA ìµœì í™”\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if hasattr(torch.backends.cuda, \"matmul\") and hasattr(\n",
    "    torch.backends.cuda.matmul, \"allow_tf32\"\n",
    "):\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# ëœë¤ ì‹œë“œ ê³ ì •\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihi5nNMSioM3"
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(batch_prompts):\n",
    "    return tokenizer(\n",
    "        batch_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZER_MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def generate_batch(batch_tokens, max_new_tokens):\n",
    "    return model.generate(\n",
    "        input_ids=batch_tokens[\"input_ids\"],\n",
    "        attention_mask=batch_tokens[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        temperature=TEMPERATURE,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_response(batch_prompts, max_new_tokens):\n",
    "    batch_tokens = tokenize_batch(batch_prompts)\n",
    "    batch_tokens = generate_batch(batch_tokens, max_new_tokens)\n",
    "    return tokenizer.batch_decode(batch_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7etKPdxWXHJ3"
   },
   "source": [
    "## ë‹µë³€ ìƒì„±\n",
    "\n",
    "0. `context`ì™€ `question`ì„ í•˜ë‚˜ë¡œ í•©ì³ `prompt.csv`ë¡œ ì €ì¥\n",
    "1. `prompt`ì—ì„œ biasê°€ ìˆëŠ”ì§€ í™•ì¸ í›„ `flag.csv`ë¡œ ì €ì¥\n",
    "2. biasê°€ ìˆëŠ” ë°ì´í„°ë§Œ ë½‘ì•„ self-debiasingë¥¼ ì‹¤í–‰í•˜ê³  `debiased.csv`ì— ì €ì¥\n",
    "3. `debiased.csv`ë¥¼ ë¶ˆëŸ¬ì™€ ì „ì²´ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ë‹µë³€ ìƒì„±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbPItaWKIDQh"
   },
   "source": [
    "### 0. prompt ìƒì„±\n",
    "\n",
    "contextì™€ questionì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "ê²°ê³¼: `prompt.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxLJUkw6IDQi"
   },
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(join_path(INPUT_DATA), encoding=\"utf-8-sig\")\n",
    "df_context_question = df_original[[\"context\", \"question\"]]\n",
    "\n",
    "prompts = [None] * len(df_context_question)\n",
    "\n",
    "\n",
    "def concat_context_question(row):\n",
    "    # context + question í•©ì¹˜ê¸°\n",
    "    return \"\\n\".join([row[\"context\"], row[\"question\"]])\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {\n",
    "        executor.submit(concat_context_question, row): idx\n",
    "        for idx, row in df_context_question.iterrows()\n",
    "    }\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        idx = futures[future]\n",
    "        prompts[idx] = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7i82syHIDQi"
   },
   "outputs": [],
   "source": [
    "df_prompt = df_original[[\"ID\", \"choices\"]].copy()\n",
    "df_prompt[\"prompt\"] = prompts\n",
    "save_csv(\n",
    "    df_prompt,\n",
    "    path=join_path(\"prompt.csv\"),\n",
    "    cols=[\"ID\", \"prompt\", \"choices\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goQdZr5lIDQi"
   },
   "source": [
    "### 1. bias í™•ì¸\n",
    "\n",
    "ê²°ê³¼: `flag.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZHHD8e5IDQi"
   },
   "outputs": [],
   "source": [
    "df_prompt = pd.read_csv(join_path(\"prompt.csv\"), encoding=\"utf-8-sig\")\n",
    "total_data_size = len(df_prompt)\n",
    "\n",
    "# Check point í™•ì¸\n",
    "check_point_path = join_path(CHECKPOINT_DIR, f\"flag_{LAST_FLAG_CHECK_POINT}.csv\")\n",
    "start_idx = LAST_FLAG_CHECK_POINT\n",
    "\n",
    "if os.path.exists(check_point_path):\n",
    "    df_check_point = pd.read_csv(check_point_path)\n",
    "else:\n",
    "    # Check pointê°€ ì—†ì„ ë•Œ ì´ˆê¸°í™”\n",
    "    df_check_point = pd.DataFrame(\n",
    "        {\n",
    "            \"pointer\": [i for i in range(total_data_size)],\n",
    "            \"flag\": [False for _ in range(total_data_size)],\n",
    "        }\n",
    "    )\n",
    "    start_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i477X_IPZlT6"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def check_bias(batch_prompt):\n",
    "    check_bias_prompt = [generate_bias_determination(prompt) for prompt in batch_prompt]\n",
    "    responses = generate_response(check_bias_prompt, max_new_tokens=16)\n",
    "    is_biased = [has_yes(trim_assistant_response(response)) for response in responses]\n",
    "    return is_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAQY-MsEIDQi"
   },
   "outputs": [],
   "source": [
    "os.makedirs(join_path(CHECKPOINT_DIR), exist_ok=True)\n",
    "collect_garbage()\n",
    "\n",
    "# í¸í–¥ í™•ì¸ ì‹œì‘\n",
    "start_time = time.time()\n",
    "while start_idx < total_data_size:\n",
    "    end_idx = min(start_idx + BATCH_SIZE, total_data_size)\n",
    "\n",
    "    batch_prompt = df_prompt.iloc[start_idx:end_idx][\"prompt\"].tolist()\n",
    "    batch_is_biased = check_bias(batch_prompt)\n",
    "\n",
    "    for idx, is_biased in enumerate(batch_is_biased):\n",
    "        idx = idx + start_idx\n",
    "        df_check_point.at[idx, \"pointer\"] = idx\n",
    "        df_check_point.at[idx, \"flag\"] = is_biased\n",
    "\n",
    "        if idx % CHECK_POINT_STEP == 0:\n",
    "            # Check pointì—ì„œ ë‹µë³€ì„ íŒŒì¼ë¡œ ì €ì¥\n",
    "            end_time = time.time()\n",
    "            save_csv(\n",
    "                df_check_point,\n",
    "                path=join_path(CHECKPOINT_DIR, f\"flag_{str(idx)}.csv\"),\n",
    "                cols=[\"pointer\", \"flag\"],\n",
    "            )\n",
    "            print(\n",
    "                f\"âœ…{idx}/{total_data_size} ì €ì¥. ({(end_time - start_time) / 60:.1f}ë¶„)\"\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    start_idx = end_idx\n",
    "\n",
    "save_csv(\n",
    "    df_check_point,\n",
    "    path=join_path(CHECKPOINT_DIR, f\"flag_final.csv\"),\n",
    "    cols=[\"pointer\", \"flag\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGrvZhZgIDQi"
   },
   "outputs": [],
   "source": [
    "# í¸í–¥ì´ ìˆë‹¤ê³  íŒë‹¨í•œ promptë§Œ ëª¨ì•„ì„œ ì €ì¥\n",
    "df_flag = pd.concat([df_prompt, df_check_point], axis=1)\n",
    "df_flag = df_flag[df_flag[\"flag\"].astype(bool) == True]\n",
    "save_csv(df_flag, path=join_path(\"flag.csv\"), cols=[\"pointer\", \"prompt\"])\n",
    "\n",
    "print(f\"í¸í–¥ì´ ìˆë‹¤ê³  íŒë‹¨í•œ í”„ë¡¬í”„íŠ¸: {len(df_flag)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YzTJGg6IDQi"
   },
   "source": [
    "### 2. prompt ì¬ìƒì„±\n",
    "\n",
    "ê²°ê³¼: `debiased.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W01agk5TIDQi"
   },
   "outputs": [],
   "source": [
    "df_flag = pd.read_csv(join_path(\"flag.csv\"))\n",
    "\n",
    "# Check point í™•ì¸\n",
    "check_point_path = join_path(CHECKPOINT_DIR, f\"debiased_{LAST_PROMPT_CHECK_POINT}.csv\")\n",
    "start_idx = LAST_PROMPT_CHECK_POINT\n",
    "\n",
    "if os.path.exists(check_point_path):\n",
    "    df_check_point = pd.read_csv(check_point_path)\n",
    "else:\n",
    "    # Check pointê°€ ì—†ì„ ë•Œ ì´ˆê¸°í™”\n",
    "    df_check_point = df_flag\n",
    "    df_check_point[[\"debiased\", \"reason\"]] = \"\"\n",
    "    start_idx = 0\n",
    "\n",
    "del df_flag\n",
    "total_data_size = len(df_check_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GM5KZcizIDQj"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_debiased_prompt(batch_prompt):\n",
    "    check_bias_prompt = [generate_bias_analysis(prompt) for prompt in batch_prompt]\n",
    "    check_bias_responses = generate_response(\n",
    "        check_bias_prompt, max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "    remove_bias_prompt = generate_cognitive_debiasing()\n",
    "    remove_bias_prompt = [\n",
    "        f\"{response}\\n{remove_bias_prompt}\" for response in check_bias_responses\n",
    "    ]\n",
    "    remove_bias_responses = generate_response(\n",
    "        remove_bias_prompt, max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "\n",
    "    return remove_bias_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPBDil_LPYlv"
   },
   "outputs": [],
   "source": [
    "os.makedirs(join_path(CHECKPOINT_DIR), exist_ok=True)\n",
    "collect_garbage()\n",
    "\n",
    "# í¸í–¥ ì—†ëŠ” í”„ë¡¬í”„íŠ¸ë¡œ ì¬ìƒì„±\n",
    "start_time = time.time()\n",
    "while start_idx < total_data_size:\n",
    "    end_idx = min(start_idx + BATCH_SIZE, total_data_size)\n",
    "\n",
    "    batch_context = df_check_point.iloc[start_idx:end_idx][\"prompt\"].tolist()\n",
    "    debiased_prompts = generate_debiased_prompt(batch_context)\n",
    "\n",
    "    for idx, debiased_prompt in enumerate(debiased_prompts):\n",
    "        idx = idx + start_idx\n",
    "        reason, prompt = debiased_prompt.rsplit(\"assistant\", 1)\n",
    "        df_check_point.at[idx, \"debiased\"] = prompt\n",
    "        df_check_point.at[idx, \"reason\"] = reason\n",
    "\n",
    "        if idx % CHECK_POINT_STEP == 0:\n",
    "            # Check pointì—ì„œ ë‹µë³€ì„ íŒŒì¼ë¡œ ì €ì¥\n",
    "            end_time = time.time()\n",
    "            save_csv(\n",
    "                df_check_point,\n",
    "                path=join_path(CHECKPOINT_DIR, f\"debiased_{str(idx)}.csv\"),\n",
    "                cols=[\"pointer\", \"prompt\", \"debiased\", \"reason\"],\n",
    "            )\n",
    "            print(\n",
    "                f\"âœ…{idx}/{total_data_size} ì €ì¥. ({(end_time - start_time) / 60:.1f}ë¶„)\"\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    start_idx = end_idx\n",
    "\n",
    "save_csv(\n",
    "    df_check_point,\n",
    "    path=join_path(CHECKPOINT_DIR, \"debiased_final.csv\"),\n",
    "    cols=[\"pointer\", \"prompt\", \"debiased\", \"reason\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OS_LsvEWIDQj"
   },
   "outputs": [],
   "source": [
    "df_prompt = pd.read_csv(join_path(\"prompt.csv\"), encoding=\"utf-8-sig\")\n",
    "\n",
    "for _, row in df_check_point.iterrows():\n",
    "    idx = row[\"pointer\"].astype(int)\n",
    "    debiased = row[\"debiased\"]\n",
    "    df_prompt.at[idx, \"prompt\"] = debiased\n",
    "\n",
    "save_csv(\n",
    "    df_prompt,\n",
    "    path=join_path(\"debiased.csv\"),\n",
    "    col=[\"ID\", \"prompt\", \"choices\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miaI2uTJIDQj"
   },
   "source": [
    "### 3. ìµœì¢… ë‹µë³€ ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMxwmIsyIDQj"
   },
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ ë°ì´í„° ì¤€ë¹„\n",
    "df_prompt = pd.read_csv(join_path(\"debiased.csv\"))\n",
    "\n",
    "# Check point í™•ì¸\n",
    "check_point_path = join_path(\n",
    "    CHECKPOINT_DIR, f\"submission_{LAST_SUBMISSION_CHECK_POINT}.csv\"\n",
    ")\n",
    "start_idx = LAST_PROMPT_CHECK_POINT\n",
    "\n",
    "if os.path.exists(check_point_path):\n",
    "    df_check_point = pd.read_csv(check_point_path)\n",
    "else:\n",
    "    # Check pointê°€ ì—†ì„ ë•Œ ì´ˆê¸°í™”\n",
    "    df_check_point = df_prompt\n",
    "    start_idx = 0\n",
    "    for col in [\"raw_input\", \"raw_output\", \"answer\"]:\n",
    "        df_check_point[col] = \"\"\n",
    "\n",
    "total_data_size = len(df_check_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s48m3gDTIDQj"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(batch_data):\n",
    "    question_prompt = [\n",
    "        generate_question(data[\"prompt\"], ast.literal_eval(data[\"choices\"]))\n",
    "        for data in batch_data\n",
    "    ]\n",
    "    responses = generate_response(question_prompt, max_new_tokens=16)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YC7jtGeIDQj"
   },
   "outputs": [],
   "source": [
    "os.makedirs(join_path(CHECKPOINT_DIR), exist_ok=True)\n",
    "collect_garbage()\n",
    "\n",
    "# ëª¨ë¸ ì¶”ë¡  ì‹œì‘\n",
    "start_time = time.time()\n",
    "while start_idx < total_data_size:\n",
    "    end_idx = min(start_idx + BATCH_SIZE, total_data_size)\n",
    "\n",
    "    batch_df = df_check_point.iloc[start_idx:end_idx][[\"prompt\", \"choices\"]]\n",
    "    responses = inference(batch_df)\n",
    "\n",
    "    for idx, response in enumerate(responses):\n",
    "        idx = idx + start_idx\n",
    "        raw_input, raw_answer = response.rsplit(\"assistant\", 1)\n",
    "        df_check_point.at[idx, \"raw_input\"] = raw_input\n",
    "        df_check_point.at[idx, \"raw_output\"] = raw_answer\n",
    "        choices = ast.literal_eval(df_original.at[idx, \"choices\"])\n",
    "        df_check_point.at[idx, \"answer\"] = extract_last_choice(raw_answer, choices)\n",
    "\n",
    "        if idx % CHECK_POINT_STEP == 0:\n",
    "            # Check pointì—ì„œ ë‹µë³€ì„ íŒŒì¼ë¡œ ì €ì¥\n",
    "            end_time = time.time()\n",
    "            save_csv(\n",
    "                df_check_point,\n",
    "                path=join_path(CHECKPOINT_DIR, f\"submission_{str(idx)}.csv\"),\n",
    "                cols=[\"ID\", \"raw_input\", \"raw_output\", \"answer\"],\n",
    "            )\n",
    "            print(\n",
    "                f\"âœ…{idx}/{total_data_size} ì €ì¥. ({(end_time - start_time) / 60:.1f}ë¶„)\"\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    start_idx = end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJQ-mHCDIDQj"
   },
   "source": [
    "## ì œì¶œ íŒŒì¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVGOi4l2PYlv"
   },
   "outputs": [],
   "source": [
    "# ìµœì¢… íŒŒì¼ ì €ì¥\n",
    "save_csv(\n",
    "    df_check_point,\n",
    "    join_path(\"submission.csv\"),\n",
    "    cols=[\"ID\", \"raw_input\", \"raw_output\", \"answer\"],\n",
    ")\n",
    "print(\"ğŸ« ê¸°ë¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
