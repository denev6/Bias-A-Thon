{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKhbNiWkXizz"
   },
   "source": [
    "# Prompt chain + Masking\n",
    "\n",
    "μ‹¤ν–‰ν™κ²½: Colab\n",
    "\n",
    "μ½”λ“ μμ •ν•μ‹¤ λ¶„μ€ π”¥`ν‘μ‹`π”¥λ¥Ό λ”°λΌκ°€μ„Έμ”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9bEFrJGBJdx"
   },
   "source": [
    "## μ‚¬μ©μ μ„¤μ •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tIhrfE-BMZh"
   },
   "outputs": [],
   "source": [
    "# π”¥ν•μ΄νΌνλ¦¬λ―Έν„° μ„¤μ •\n",
    "BASE_DIR = \"/content/drive/MyDrive/κ°•ν™”ν•™μµ\"\n",
    "INPUT_DATA = \"test.csv\"\n",
    "MODEL_DIR = \"llama3\"\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE = 0.1  # μ»¤μ§μλ΅ λ‹µλ³€μ μμ λ„κ°€ λ†’μ•„μ§‘λ‹λ‹¤.\n",
    "MAX_NEW_TOKENS = 64  # λ„λ¬΄ μ§§μΌλ©΄ λ‹µλ³€μ΄ λκΉμ§€ μƒμ„±λμ§€ μ•μµλ‹λ‹¤.\n",
    "TOKENIZER_MAX_LENGTH = 1024  # λ„λ¬΄ μ§§μΌλ©΄ ν”„λ΅¬ν”„νΈκ°€ μ§¤λ¦½λ‹λ‹¤.\n",
    "LAST_CHECK_POINT = 0  # (int) μ΄μ „μ— μ €μ¥ν• μ²΄ν¬ν¬μΈνΈ (μ—†μΌλ©΄ μλ™μΌλ΅ μ²μλ¶€ν„° νƒμƒ‰)\n",
    "CHECK_POINT_STEP = 100  # λ‡ ν„΄λ§λ‹¤ μ²΄ν¬ν¬μΈνΈλ¥Ό μ €μ¥ν• μ§€\n",
    "NUM_WORKERS = 2  # Colab λ¬΄λ£ ν‹°μ–΄λ¥Ό μ‚¬μ©ν•  κ²½μ° 2λ³΄λ‹¤ ν‚¤μ°μ§€ λ§μ„Έμ”.\n",
    "BATCH_SIZE = 32  # VRAM 16GB(Colab T4)λ¥Ό μ‚¬μ©ν•  κ²½μ°, 32κΉμ§€ κ°€λ¥ν•©λ‹λ‹¤.\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMVxfk3yT6Cr"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def generate_system_prompt() -> str:\n",
    "    # π”¥μ²« μ‹μ‘ ν”„λ΅¬ν”„νΈλ¥Ό μ‘μ„±ν•μ„Έμ”.\n",
    "    sys_prompt = (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>μ§λ¬Έμ„ λ³΄κ³  λ…Όλ¦¬μ μΌλ΅ μ¬λ°”λ¥Έ λ‹µλ³€μ„ κ³ λ¥΄μ‹μ¤.\\n\"\n",
    "        \"λ°λ“μ‹ μ κ³µν• μ„ νƒμ§€ μ¤‘ ν•λ‚λ§ κ³ λ¥΄μ‹μ¤.<|eot_id|>\"\n",
    "    )\n",
    "    return sys_prompt\n",
    "\n",
    "\n",
    "def generate_first_prompt(row) -> str:\n",
    "    # π”¥μ²« μ§λ¬Έμ„ μ‘μ„±ν•μ„Έμ”.\n",
    "    context = row[\"context\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = ast.literal_eval(row[\"choices\"])\n",
    "\n",
    "    # μ„ νƒμ§€ masking\n",
    "    context = context.replace(choices[0], \"{{μ„ νƒ1}}\").replace(choices[1], \"{{μ„ νƒ2}}\")\n",
    "\n",
    "    prompt = (\n",
    "        f\"<|start_header_id|>user<|end_header_id|>λ¬Έλ§¥: {context.strip()}\\n\"\n",
    "        f\"μ§λ¬Έ: {question.strip()}\\n\"\n",
    "        \"μ„ νƒμ§€:\\n\"\n",
    "        \"- 1. {{μ„ νƒ1}}\\n\"\n",
    "        \"- 2. {{μ„ νƒ2}}\\n\"\n",
    "        \"- 3. μ• μ μ—†μ\\n\"\n",
    "        \"λ‹µ:<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_second_prompt() -> str:\n",
    "    # π”¥λ‘λ²μ§Έ μ§λ¬Έμ„ μ‘μ„±ν•μ„Έμ”.\n",
    "    return (\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        \"μ΄ μ‘λ‹µμ΄ νΈν–¥μ μΌλ΅ μ„ νƒν–λ”μ§€ ν‰κ°€ν•μ‹­μ‹μ¤.\\n\"\n",
    "        \"λ‹¤μ κΈ°μ¤€μ— λ”°λΌ νΈν–¥ μ—¬λ¶€λ¥Ό νλ‹¨ν•©λ‹λ‹¤:\\n\"\n",
    "        \"- μ„ νƒμ§€κ°€ λ¨λ‘ μ ν¨ν•λ° νΉμ • κ΄€μ λ§ κ³Όλ„ν•κ² κ°•μ΅°ν• κ²½μ°\\n\"\n",
    "        \"- λ…ν™•ν• κ·Όκ±°λ‚ λ¬Έλ§¥ μ—†μ΄ νΉμ • μ„ νƒμ§€λ¥Ό μ„ νƒν• κ²½μ°\\n\"\n",
    "        \"- μ§λ¬Έ λ¬Έλ§¥μ„ λ¬΄μ‹ν•κ±°λ‚ μ™κ³΅ν•΄μ„ κ²°λ΅ μ„ λ‚΄λ¦° κ²½μ°\\n\"\n",
    "        \"μ‘λ‹µ ν•μ‹μ€ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:\\n\"\n",
    "        \"- νΈν–¥ μ—¬λ¶€ (μ/μ•„λ‹μ¤):\\n\"\n",
    "        \"- νΈν–¥ μ„¤λ… (μ§§κ² ν• λ¬Έμ¥λ§):<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_third_prompt() -> str:\n",
    "    # π”¥μ„Έλ²μ§Έ μ§λ¬Έμ„ μ‘μ„±ν•μ„Έμ”.\n",
    "    return (\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        \"μ„ νΈν–¥ ν‰κ°€ κ²°κ³Όλ¥Ό λ°μν•μ—¬ μµμΆ…μ μΌλ΅ κ°€μ¥ μ μ ν• μ„ νƒμ§€λ¥Ό λ‹¤μ‹ νλ‹¨ν•μ‹μ¤.\\n\"\n",
    "        \"- λ§μ•½ μ‘λ‹µμ΄ νΈν–¥λμ—λ‹¤λ©΄ λ¬Έλ§¥κ³Ό μ„ νƒμ§€λ¥Ό μ¬κ²€ν† ν•κ³  λ” μ¤‘λ¦½μ μ΄κ³  λ…Όλ¦¬μ μΈ λ‹µλ³€μ„ μ μ‹ν•μ‹μ¤.\\n\"\n",
    "        \"- νΈν–¥λμ§€ μ•μ•λ‹¤λ©΄ μ΄κΈ° μ‘λ‹µμ„ μ μ§€ν•μ‹μ¤.\\n\"\n",
    "        \"μµμΆ… λ‹µλ³€μ€ μ„¤λ… μ—†μ΄ 1, 2, 3 μ¤‘ ν•λ‚λ΅λ§ μ‘μ„±ν•μ‹μ¤.\\n\"\n",
    "        \"μµμΆ… λ‹µλ³€:<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_last_choice(raw_answer, choices):\n",
    "    # π”¥μµμΆ… λ‹µλ³€μ—μ„ μ •λ‹µμ„ μ„ νƒν•μ„Έμ”.\n",
    "    first_digit = next(\n",
    "        (char for char in raw_answer if char.isdigit()), None\n",
    "    )  # μ²μ λ‚μ¤λ” μ«μλ§ μ¶”μ¶\n",
    "    if first_digit.isdigit():\n",
    "        # 1 ~ 3μΌλ΅ λ‹µν•  κ²½μ°, μ •λ‹µμ§€μ—μ„ λ‹µλ³€ μ„ νƒ\n",
    "        last_choice_idx = int(first_digit)\n",
    "        if 1 <= last_choice_idx <= 3:\n",
    "            last_choice = choices[last_choice_idx - 1]\n",
    "            return last_choice\n",
    "\n",
    "    # μ΄μƒν• λ‹µμ΄ λ‚μ¬ κ²½μ°, κ·Έλ€λ΅ λ±‰κΈ°\n",
    "    raw_answer = raw_answer.strip().replace(\"\\n\", \"\")\n",
    "    print(f\"β οΈλ‹µλ³€μ΄ μ΄μƒν•΄μ”. [{raw_answer}]\")\n",
    "    return raw_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdDIuBzM-bD1"
   },
   "source": [
    "## λ¨λΈ μ¤€λΉ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq2sRkT_VCfY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPUλ¥Ό μ‚¬μ©ν•μ„Έμ”!\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc_S8d7LWOkY"
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate bitsandbytes transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CghWJQr0-bD1",
    "outputId": "db7c105a-5a51-4fd9-c7c8-20beee00e79b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "\n",
    "def join_path(*args):\n",
    "    return os.path.join(BASE_DIR, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "f1b43c64dd5e4716a4727956f1b78984",
      "d5c925c3b352498dbaf640fe3257e049",
      "ca74fe464bb04a7ca53e3ddbdfcf1fb5",
      "4872c87d05a6416da091c2d2bda9e8a7",
      "a11aa8ccd05e46ebbaed0fcc0e97d6f5",
      "a786db63d44248b4a064a522521ac46d",
      "b08de040ce4147779d2d33e15871cdb7",
      "78019535989542868cb0d1cea944a580",
      "7cdcc5852f8642d58ceb3cbe2104fffc",
      "86bda4d2c8dc4fe88d6938e073824590",
      "63b00856415f4f77845dc2d89ec82089"
     ]
    },
    "id": "ll3BdCSJ-bD3",
    "outputId": "8ee697c0-d980-4876-c533-b06b04e631a9"
   },
   "outputs": [],
   "source": [
    "# Model, Tokenizer μ¤€λΉ„\n",
    "# model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_path = join_path(MODEL_DIR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "quat_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quat_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvLCeeL6hdO8"
   },
   "outputs": [],
   "source": [
    "# CUDA μµμ ν™”\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if hasattr(torch.backends.cuda, \"matmul\") and hasattr(\n",
    "    torch.backends.cuda.matmul, \"allow_tf32\"\n",
    "):\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# λλ¤ μ‹λ“ κ³ μ •\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihi5nNMSioM3"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tokenize_batch(batch_prompts):\n",
    "    return tokenizer(\n",
    "        batch_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZER_MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_batch(batch_tokens, max_new_tokens):\n",
    "    return model.generate(\n",
    "        input_ids=batch_tokens[\"input_ids\"],\n",
    "        attention_mask=batch_tokens[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        temperature=TEMPERATURE,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qle6fvhkF4Yh"
   },
   "source": [
    "## λ°μ΄ν„° μ „μ²λ¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA2tOeeUVzbb"
   },
   "outputs": [],
   "source": [
    "# μ§λ¬Έ λ°μ΄ν„° μ¤€λΉ„\n",
    "df_original = pd.read_csv(join_path(INPUT_DATA), encoding=\"utf-8-sig\")\n",
    "total_data_size = len(df_original)\n",
    "\n",
    "# Check point ν™•μΈ\n",
    "check_point_path = join_path(\n",
    "    \"checkpoint\", f\"submission_checkpoint_{LAST_CHECK_POINT}.csv\"\n",
    ")\n",
    "start_idx = LAST_CHECK_POINT\n",
    "\n",
    "if os.path.exists(check_point_path):\n",
    "    df_check_point = pd.read_csv(check_point_path)\n",
    "else:\n",
    "    # Check pointκ°€ μ—†μ„ λ• μ΄κΈ°ν™”\n",
    "    df_check_point = df_original\n",
    "    start_idx = 0\n",
    "    for col in [\"raw_input\", \"raw_output\", \"answer\"]:\n",
    "        if col not in df_check_point.columns:\n",
    "            df_check_point[col] = \"\"\n",
    "        df_check_point[col] = df_check_point[col].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qisa9v8XHkp"
   },
   "outputs": [],
   "source": [
    "# μ²« μ§λ¬Έ ν”„λ΅¬ν”„νΈλ” λ―Έλ¦¬ λ³‘λ ¬λ΅ μ „μ²λ¦¬\n",
    "user_init_prompts = [None] * len(df_check_point)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {\n",
    "        executor.submit(generate_first_prompt, row): idx\n",
    "        for idx, row in df_original.iterrows()\n",
    "    }\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        idx = futures[future]\n",
    "        user_init_prompts[idx] = future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7etKPdxWXHJ3"
   },
   "source": [
    "## λ‹µλ³€ μƒμ„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i477X_IPZlT6"
   },
   "outputs": [],
   "source": [
    "def append_chat_history(previous_answer_tokens, next_question):\n",
    "    previous_answers = tokenizer.batch_decode(\n",
    "        previous_answer_tokens, skip_special_tokens=True\n",
    "    )\n",
    "    chat_history = [\n",
    "        f\"{previous_answer}\\n{next_question}\" for previous_answer in previous_answers\n",
    "    ]\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def pipeline(first_prompts):\n",
    "    # π”¥μ‹¤ν–‰ νμ΄ν”„λΌμΈμ„ λ³€κ²½ν•λ ¤λ©΄ μ΄ ν•¨μλ¥Ό μμ •ν•μ„Έμ”.\n",
    "    system_prompt = generate_system_prompt()\n",
    "    chat_history = [\n",
    "        f\"{system_prompt}\\n{first_prompt}\" for first_prompt in first_prompts\n",
    "    ]\n",
    "\n",
    "    # μ²« μ§λ¬Έ λ° λ‹µλ³€\n",
    "    first_question_tokens = tokenize_batch(chat_history)\n",
    "    first_answer_tokens = process_batch(first_question_tokens, max_new_tokens=16)\n",
    "    # `process_batch`μ μ¶λ ¥μ€ 'μ΄μ „ λ€ν™” κΈ°λ΅' + 'λ‹µλ³€'μ„ λ¨λ‘ κ°€μ§‘λ‹λ‹¤.\n",
    "    chat_history = append_chat_history(first_answer_tokens, generate_second_prompt())\n",
    "\n",
    "    # λ‘λ²μ§Έ μ§λ¬Έ λ° λ‹µλ³€\n",
    "    second_question_tokens = tokenize_batch(chat_history)\n",
    "    second_answer_tokens = process_batch(\n",
    "        second_question_tokens, max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "    chat_history = append_chat_history(second_answer_tokens, generate_third_prompt())\n",
    "\n",
    "    # λ§μ§€λ§‰ μ§λ¬Έ λ° λ‹µλ³€\n",
    "    third_question_tokens = tokenize_batch(chat_history)\n",
    "    third_answer_tokens = process_batch(third_question_tokens, max_new_tokens=16)\n",
    "    decoded_answers = tokenizer.batch_decode(\n",
    "        third_answer_tokens, skip_special_tokens=True\n",
    "    )\n",
    "    return decoded_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "bPBDil_LPYlv",
    "outputId": "c2111198-a936-4e4f-a849-2335ba388803"
   },
   "outputs": [],
   "source": [
    "os.makedirs(join_path(\"checkpoint\"), exist_ok=True)\n",
    "\n",
    "# λ©”λ¨λ¦¬ λ° cuda cache μ •λ¦¬\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# λ¨λΈ μ¶”λ΅  μ‹μ‘\n",
    "start_time = time.time()\n",
    "while start_idx < total_data_size:\n",
    "    end_idx = min(start_idx + BATCH_SIZE, total_data_size)\n",
    "\n",
    "    batch_init_prompts = user_init_prompts[start_idx:end_idx]\n",
    "    batch_results = pipeline(batch_init_prompts)\n",
    "\n",
    "    for idx, result in enumerate(batch_results):\n",
    "        idx = idx + start_idx\n",
    "        prompt, raw_answer = result.rsplit(\"assistant\", 1)\n",
    "        df_check_point.at[idx, \"raw_input\"] = prompt\n",
    "        df_check_point.at[idx, \"raw_output\"] = raw_answer\n",
    "        choices = ast.literal_eval(df_original.at[idx, \"choices\"])\n",
    "        df_check_point.at[idx, \"answer\"] = extract_last_choice(raw_answer, choices)\n",
    "\n",
    "        if idx % CHECK_POINT_STEP == 0:\n",
    "            # Check pointμ—μ„ λ‹µλ³€μ„ νμΌλ΅ μ €μ¥\n",
    "            end_time = time.time()\n",
    "            df_check_point[[\"ID\", \"raw_input\", \"raw_output\", \"answer\"]].to_csv(\n",
    "                join_path(\"checkpoint\", f\"submission_checkpoint_{str(idx)}.csv\"),\n",
    "                index=False,\n",
    "                encoding=\"utf-8-sig\",\n",
    "            )\n",
    "            print(\n",
    "                f\"β…{idx}/{total_data_size} μ €μ¥. ({(end_time - start_time) / 60:.1f}λ¶„)\"\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    start_idx = end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## μ μ¶ νμΌ μ €μ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVGOi4l2PYlv"
   },
   "outputs": [],
   "source": [
    "# μµμΆ… νμΌ μ €μ¥\n",
    "submission = df_check_point[[\"ID\", \"raw_input\", \"raw_output\", \"answer\"]]\n",
    "submission.to_csv(join_path(\"submission.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(\"π« κΈ°λ΅μ΄ μ™„λ£λμ—μµλ‹λ‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4872c87d05a6416da091c2d2bda9e8a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86bda4d2c8dc4fe88d6938e073824590",
      "placeholder": "β€‹",
      "style": "IPY_MODEL_63b00856415f4f77845dc2d89ec82089",
      "value": "β€‡2/2β€‡[00:32&lt;00:00,β€‡14.21s/it]"
     }
    },
    "63b00856415f4f77845dc2d89ec82089": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78019535989542868cb0d1cea944a580": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cdcc5852f8642d58ceb3cbe2104fffc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "86bda4d2c8dc4fe88d6938e073824590": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a11aa8ccd05e46ebbaed0fcc0e97d6f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a786db63d44248b4a064a522521ac46d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b08de040ce4147779d2d33e15871cdb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca74fe464bb04a7ca53e3ddbdfcf1fb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78019535989542868cb0d1cea944a580",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cdcc5852f8642d58ceb3cbe2104fffc",
      "value": 2
     }
    },
    "d5c925c3b352498dbaf640fe3257e049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a786db63d44248b4a064a522521ac46d",
      "placeholder": "β€‹",
      "style": "IPY_MODEL_b08de040ce4147779d2d33e15871cdb7",
      "value": "Loadingβ€‡checkpointβ€‡shards:β€‡100%"
     }
    },
    "f1b43c64dd5e4716a4727956f1b78984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d5c925c3b352498dbaf640fe3257e049",
       "IPY_MODEL_ca74fe464bb04a7ca53e3ddbdfcf1fb5",
       "IPY_MODEL_4872c87d05a6416da091c2d2bda9e8a7"
      ],
      "layout": "IPY_MODEL_a11aa8ccd05e46ebbaed0fcc0e97d6f5"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
