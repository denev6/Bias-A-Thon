"""Reasoning 5-shot learning with temperature adjustment v2"""

import ast
import time

from config import *
from model import Model, set_cuda, collect_garbage
from prompt import (
    preprocess,
    generate_full_prompt,
    split_answer,
    extract_last_choice,
)
from utils import ignore_warnings, load_data, save_data, join_path


# ìˆ˜í–‰í•  ì‘ì—…
def pipeline(model, batch_prompts, max_new_tokens) -> list[str]:
    question_tokens = model.tokenize_batch(batch_prompts)
    answer_tokens = model.process_batch(question_tokens, max_new_tokens)
    return answer_tokens


def compute_reward_length_based(
    output: str, min_len: int = 60, max_len: int = 110
) -> int:
    """ë‹µë³€ ê¸¸ì´ì— ë”°ë¼ ë³´ìƒ ê³„ì‚°"""
    length = len(output)
    if length < min_len or length > max_len:
        return -1  # ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸¸ë©´ íŒ¨ë„í‹°
    return 1  # ì ë‹¹í•˜ë©´ ë³´ìƒ


print("ğŸ”¥ì„¤ì • ì¤€ë¹„ ì¤‘...")
if IGNORE_WARNING:
    ignore_warnings()

# CUDA ì„¤ì •
device = set_cuda(RANDOM_SEED)

# Llama3
print("ğŸ”¥ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
our_llm = Model(
    join_path(MODEL_DIR),
    device=device,
    max_length=TOKENIZER_MAX_LENGTH,
    do_sample=DO_SAMPLE,
    temperature=TEMPERATURE,
    top_k=TOP_K,
    skip_special_tokens=SKIP_SPECIAL_TOKENS,
    device_map=MODEL_DEVICE_MAP,
)

#######################################


file_prefix = "submit"

df_original, df_check_point, start_idx = load_data(
    path=join_path(INPUT_CSV),
    cols=["ID", "raw_input", "raw_output", "answer"],
    checkpoint_dir=join_path(CHECKPOINT_DIR),
    last_checkpoint=LAST_INFERENCE_CHECK_POINT,
    prefix=file_prefix,
)
total_data_size = len(df_check_point)

# ë§ˆìŠ¤í‚¹ëœ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
prompts = preprocess(
    df_original,
    function=generate_full_prompt,
    num_workers=NUM_WORKERS,
)

# ì¶”ë¡  ë° ì €ì¥
print("ğŸ”¥ëª¨ë¸ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
collect_garbage()

start_time = time.time()

current_temperature = TEMPERATURE
min_temperature = 0.3

while start_idx < total_data_size:
    end_idx = min(start_idx + BATCH_SIZE, total_data_size)
    batch_prompts = prompts[start_idx:end_idx]
    batch_answers = pipeline(our_llm, batch_prompts, max_new_tokens=MAX_NEW_TOKENS)

    for idx, answer in enumerate(batch_answers):
        idx = idx + start_idx

        # ëª¨ë¸ì˜ ìµœì¢… ì‘ë‹µì—ì„œ ì „ì²´ ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        prompt, raw_answer = split_answer(answer)
        # ê¸¸ì´ì— ë”°ë¥¸ ë³´ìƒ ê³„ì‚°
        reward = compute_reward_length_based(raw_answer)
        choices = ast.literal_eval(df_original.at[idx, "choices"])
        extracted_answer = extract_last_choice(raw_answer, choices)

        # ê²°ê³¼ ê¸°ë¡
        df_check_point.at[idx, "raw_input"] = prompt
        df_check_point.at[idx, "raw_output"] = raw_answer
        df_check_point.at[idx, "answer"] = extracted_answer
        df_check_point.at[idx, "reward"] = reward

        # [NEW] ì˜¨ë„ ì¡°ì • (ë³´ìˆ˜ì ìœ¼ë¡œ)
        if reward == -1:
            current_temperature = max(current_temperature * 0.9, min_temperature)
            our_llm.temperature = current_temperature
        else:
            current_temperature = min(current_temperature * 1.05, 1.0)
            our_llm.temperature = current_temperature

        if idx % CHECK_POINT_STEP == 0:
            # Check pointì—ì„œ ë‹µë³€ì„ íŒŒì¼ë¡œ ì €ì¥
            end_time = time.time()
            save_data(
                df_check_point,
                cols=["ID", "raw_input", "raw_output", "answer"],
                path=join_path(CHECKPOINT_DIR, f"{file_prefix}_{str(idx)}.csv"),
            )
            print(f"âœ…[{idx}/{total_data_size}] {(end_time - start_time) / 60:.1f}ë¶„")
            start_time = time.time()

    start_idx = end_idx


# ìµœì¢… íŒŒì¼ ì €ì¥
save_data(
    df_check_point,
    cols=["ID", "raw_input", "raw_output", "answer"],
    path=join_path(FINAL_CSV),
)
print("\nğŸ« ê¸°ë¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
