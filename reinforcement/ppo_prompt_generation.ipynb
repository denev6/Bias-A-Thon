{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4DpTEsmfSZg"
   },
   "source": [
    "# Prompt generation with PPO algorithm\n",
    "\n",
    "- `PPO`: Proximal policy optimization\n",
    "- ê°•í™”í•™ìŠµ ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzvSLLIpliOL"
   },
   "outputs": [],
   "source": [
    "# ğŸ”¥í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BASE_DIR = \"/content/drive/MyDrive/ê°•í™”í•™ìŠµ\"\n",
    "REWARD_MODEL_DIR = \"llama3\"\n",
    "POLICY_MODEL_DIR = \"exaone\"\n",
    "TRAINED_OUTPUT = \"ppo_exaone\"\n",
    "TRAIN_DATA = \"train.csv\"\n",
    "SAMPLE_RESULT_CSV = \"result_prompt.csv\"\n",
    "RANDOM_SEED = 42\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_ITER = 10\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_STEP = 1\n",
    "LOG_STEP = 100\n",
    "TOKENIZER_MAX_LENGTH = 1024\n",
    "POLICY_MAX_NEW_TOKENS = 1024\n",
    "POLICY_DO_SAMPLE = True\n",
    "POLICY_TEMPERATURE = 0.1\n",
    "REWARD_MAX_NEW_TOKENS = 32\n",
    "REWARD_DO_SAMPLE = False\n",
    "REWARD_TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2rqEE6FL2JP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CUDA ë””ë²„ê¹…\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uat-01SqM5MY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPUë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxpltCKB5lLZ"
   },
   "outputs": [],
   "source": [
    "!pip install -qq \\\n",
    "  fsspec==2025.3.2 \\\n",
    "  accelerate bitsandbytes \\\n",
    "  transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhVUTw7Y5y-J"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import ast\n",
    "import random\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import pandas as pd\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "\n",
    "def join_path(*args):\n",
    "    return os.path.join(BASE_DIR, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsAgtQlUM5MZ"
   },
   "outputs": [],
   "source": [
    "# CUDA ìµœì í™”\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if hasattr(torch.backends.cuda, \"matmul\") and hasattr(\n",
    "    torch.backends.cuda.matmul, \"allow_tf32\"\n",
    "):\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# ëœë¤ ì‹œë“œ ê³ ì •\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í”„ë¡¬í”„íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy_prompt() -> list:\n",
    "    # EXAONEì—ì„œ ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸\n",
    "    sys_prompt = \"ë‹¹ì‹ ì€ LLMì´ ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì„ íƒì§€ë¥¼ ê°ì • ì—†ì´ ì¤‘ë¦½ì ì´ê³  ë…¼ë¦¬ì ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"\n",
    "    user_prompt = (\n",
    "        \"ì¡°ê±´:\\n\"\n",
    "        \"- í”„ë¡¬í”„íŠ¸ëŠ” í•œ ë¬¸ì¥ì´ì–´ì•¼ í•˜ë©°, ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\\n\"\n",
    "        \"- í¸í–¥, ê°ì • í‘œí˜„ ì—†ì´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì‚¬ê³ ë¥¼ ìœ ë„í•´ì•¼ í•©ë‹ˆë‹¤.\\n\"\n",
    "        \"- [ë§¥ë½], [ì§ˆë¬¸], [ì„ íƒì§€]ê°€ ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§ˆ ì˜ˆì •ì´ë¯€ë¡œ, ì´ë¥¼ ê³ ë ¤í•´ ì¼ë°˜í™”ëœ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.\\n\"\n",
    "        \"- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\\n\"\n",
    "        \"- í”„ë¡¬í”„íŠ¸ë§Œ ì¶œë ¥í•˜ê³ , ê·¸ ì™¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_reward_prompt(instruction, context, question, choices) -> str:\n",
    "    # Llama3ì—ì„œ ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸\n",
    "    choices_str = \"\\n\".join([f\"{i}. {choice}\" for i, choice in enumerate(choices, 1)])\n",
    "    return (\n",
    "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
    "        f\"{instruction}<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"ë§¥ë½: {context.strip()}\\n\"\n",
    "        f\"ì§ˆë¬¸: {question.strip()}\\n\"\n",
    "        \"ì„ íƒì§€:\\n\"\n",
    "        f\"{choices_str}\\n\"\n",
    "        \"ìµœì¢… ë‹µë³€ì€ ì„¤ëª… ì—†ì´ 1, 2, 3 ì¤‘ í•˜ë‚˜ë¡œë§Œ ì‘ì„±í•˜ì‹œì˜¤.\\n\"\n",
    "        \"ìµœì¢… ë‹µë³€:<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_last_choice(raw_answer, choices) -> str:\n",
    "    first_digit = next(\n",
    "        (char for char in raw_answer if char.isdigit()), None\n",
    "    )  # ì²˜ìŒ ë‚˜ì˜¤ëŠ” ìˆ«ìë§Œ ì¶”ì¶œ\n",
    "    if first_digit.isdigit():\n",
    "        last_choice_idx = int(first_digit)\n",
    "        if 1 <= last_choice_idx <= 3:\n",
    "            last_choice = choices[last_choice_idx - 1]\n",
    "            return last_choice\n",
    "\n",
    "    raw_answer = raw_answer.strip().replace(\"\\n\", \"\")\n",
    "    return raw_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPHaTNlv711L"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ\n",
    "quat_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(join_path(POLICY_MODEL_DIR))\n",
    "if policy_tokenizer.pad_token_id is None:\n",
    "    policy_tokenizer.pad_token_id = policy_tokenizer.eos_token_id\n",
    "\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    join_path(POLICY_MODEL_DIR),\n",
    "    quantization_config=quat_config,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# meta-llama/Llama-3.1-8B-Instruct\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    join_path(REWARD_MODEL_DIR), padding_side=\"left\"\n",
    ")\n",
    "if reward_tokenizer.pad_token_id is None:\n",
    "    reward_tokenizer.pad_token_id = reward_tokenizer.eos_token_id\n",
    "\n",
    "reward_model = AutoModelForCausalLM.from_pretrained(\n",
    "    join_path(REWARD_MODEL_DIR),\n",
    "    quantization_config=quat_config,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAqm3domoKzh"
   },
   "outputs": [],
   "source": [
    "def generate_prompt_with_policy(messages, policy_model, policy_tokenizer, device):\n",
    "    input_ids = policy_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZER_MAX_LENGTH,\n",
    "    ).to(device)\n",
    "    response_id = policy_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=POLICY_MAX_NEW_TOKENS,\n",
    "        tokenizer=policy_tokenizer,\n",
    "        do_sample=POLICY_DO_SAMPLE,\n",
    "        temperature=POLICY_TEMPERATURE,\n",
    "        eos_token_id=policy_tokenizer.eos_token_id,\n",
    "        pad_token_id=policy_tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )[0]\n",
    "    action_result = (\n",
    "        policy_tokenizer.decode(response_id)\n",
    "        .split(\"[|assistant|]\")[-1]\n",
    "        .split(\"[|endofturn|]\")[0]\n",
    "        .strip()\n",
    "    )\n",
    "    return input_ids, response_id, action_result\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def interact_with_reward_model(\n",
    "    prompt, choices, true_answer, reward_model, reward_tokenizer, device\n",
    "):\n",
    "    input_ids = reward_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZER_MAX_LENGTH,\n",
    "    ).to(device)\n",
    "    response_id = reward_model.generate(\n",
    "        **input_ids,\n",
    "        max_new_tokens=REWARD_MAX_NEW_TOKENS,\n",
    "        do_sample=REWARD_DO_SAMPLE,\n",
    "        temperature=REWARD_DO_SAMPLE,\n",
    "        eos_token_id=reward_tokenizer.eos_token_id,\n",
    "        pad_token_id=reward_tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "        use_cache=True,\n",
    "    )[0]\n",
    "    result_state = (\n",
    "        reward_tokenizer.decode(response_id, skip_special_tokens=True)\n",
    "        .replace(prompt, \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    result_state = extract_last_choice(result_state, choices)\n",
    "    # ì •ë‹µì´ë©´ reward = +1, ì•„ë‹ˆë©´ -1\n",
    "    reward = 1.0 if result_state == true_answer else -1.0\n",
    "    return result_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_set(row):\n",
    "    context = row[\"context\"].strip()\n",
    "    question = row[\"question\"].strip()\n",
    "    choices = ast.literal_eval(row[\"choices\"])\n",
    "    true_answer = row[\"answer\"].strip()\n",
    "    return context, question, choices, true_answer\n",
    "\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "df_train = pd.read_csv(join_path(TRAIN_DATA), encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9r6IwdfM5Ma"
   },
   "outputs": [],
   "source": [
    "class PPOBatch:\n",
    "    def __init__(self, max_batch_size, device=\"cuda\"):\n",
    "        self.queries = []\n",
    "        self.responses = []\n",
    "        self.rewards = []\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def clear(self):\n",
    "        self.queries.clear()\n",
    "        self.responses.clear()\n",
    "        self.rewards.clear()\n",
    "\n",
    "    def append(self, query, response, reward):\n",
    "        assert len(self) <= self.max_batch_size, \"Batch size limit exceeded.\"\n",
    "        self.queries.append(query.squeeze(0))\n",
    "        self.responses.append(response.squeeze(0))\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def get_all(self):\n",
    "        reward_tensors = [\n",
    "            torch.tensor(reward, dtype=torch.float32).to(self.device)\n",
    "            for reward in self.rewards\n",
    "        ]\n",
    "        return self.queries, self.responses, reward_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gs3Ku2XNS_w"
   },
   "outputs": [],
   "source": [
    "# PPO í•™ìŠµ ì„¤ì •\n",
    "# PPOConfigì™€ PPOTrainerê°€ deprecated ë²„ì „ì„ì„ ì•Œê³  ìˆì§€ë§Œ\n",
    "# v2ëŠ” ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ê³¼ ì¶©ëŒì´ ìˆì–´ v1ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=policy_model,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=GRADIENT_STEP,\n",
    "    output_dir=join_path(\"checkpoint\"),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=LOG_STEP,\n",
    "    save_total_limit=3,\n",
    "    log_with=None,\n",
    ")\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    tokenizer=policy_tokenizer,\n",
    "    dataset=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZBy54_V899a"
   },
   "outputs": [],
   "source": [
    "os.makedirs(join_path(\"checkpoint\"), exist_ok=True)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# PPO í•™ìŠµ\n",
    "total_reward = 0\n",
    "num_total_reward = 0\n",
    "\n",
    "for epoch in range(1, MAX_ITER + 1):\n",
    "    ppo_batch = PPOBatch(BATCH_SIZE, device)\n",
    "\n",
    "    for idx, row in df_train.iterrows():\n",
    "        context, question, choices, true_answer = get_train_set(row)\n",
    "\n",
    "        # Policyë¥¼ í†µí•´ í”„ë¡¬í”„íŠ¸(action) ìƒì„±\n",
    "        train_prompt = generate_policy_prompt()\n",
    "        input_tokens, result_prompt_tokens, result_prompt = generate_prompt_with_policy(\n",
    "            train_prompt, policy_model, policy_tokenizer, device\n",
    "        )\n",
    "        # print(\"===== Action =====\\n\", result_prompt)\n",
    "\n",
    "        # Actionë¥¼ í†µí•´ reward ìƒì„±\n",
    "        result_prompt = generate_reward_prompt(\n",
    "            result_prompt, context, question, choices\n",
    "        )\n",
    "        llm_answer, reward = interact_with_reward_model(\n",
    "            result_prompt, choices, true_answer, reward_model, reward_tokenizer, device\n",
    "        )\n",
    "        # print(\"===== State =====\\n\", llm_answer)\n",
    "        # print(\"===== Reward =====\\n\", reward, \"\\n\")\n",
    "\n",
    "        # ì •ë³´ ê¸°ë¡\n",
    "        total_reward += reward\n",
    "        num_total_reward += 1\n",
    "        ppo_batch.append(input_tokens, result_prompt_tokens, reward)\n",
    "\n",
    "        if len(ppo_batch) == BATCH_SIZE:\n",
    "            # íŒŒë¼ë¯¸í„° í•™ìŠµ\n",
    "            queries, responses, rewards = ppo_batch.get_all()\n",
    "            ppo_trainer.step(queries, responses, rewards)\n",
    "            ppo_batch.clear()\n",
    "\n",
    "        if idx % LOG_STEP == 0:\n",
    "            # í•™ìŠµ í˜„í™© ì¶œë ¥\n",
    "            avg_reward = total_reward / num_total_reward\n",
    "            print(f\"[{epoch}_{idx}] Average reward: {avg_reward:.3f}\")\n",
    "\n",
    "            total_reward = 0\n",
    "            num_total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCgWLg3OM5Mb"
   },
   "outputs": [],
   "source": [
    "ppo_trainer.model.save_pretrained(join_path(TRAINED_OUTPUT))\n",
    "policy_tokenizer.save_pretrained(join_path(TRAINED_OUTPUT))\n",
    "print(\"ğŸ« í•™ìŠµì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y3MY6ScM5Mb"
   },
   "outputs": [],
   "source": [
    "def random_sample(df):\n",
    "    radom_idx = random.randint(0, len(df) - 1)\n",
    "    row = df.iloc[radom_idx]\n",
    "    context = row[\"context\"].strip()\n",
    "    question = row[\"question\"].strip()\n",
    "    choices = ast.literal_eval(row[\"choices\"])\n",
    "    true_answer = row[\"answer\"].strip()\n",
    "    return context, question, choices, true_answer\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sampled_result_as_dataframe(\n",
    "    df,\n",
    "    policy_model,\n",
    "    policy_tokenizer,\n",
    "    reward_model,\n",
    "    reward_tokenizer,\n",
    "    save_path=None,\n",
    "    num_sample=10,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    prompt_list = []\n",
    "    llm_answer_list = []\n",
    "    correct_answer_list = []\n",
    "\n",
    "    for i in range(num_sample):\n",
    "        context, question, choices, true_answer = random_sample(df)\n",
    "        train_prompt = generate_policy_prompt()\n",
    "        _, _, result_prompt = generate_prompt_with_policy(\n",
    "            train_prompt, policy_model, policy_tokenizer, device\n",
    "        )\n",
    "        result_prompt = generate_reward_prompt(\n",
    "            result_prompt, context, question, choices\n",
    "        )\n",
    "        llm_answer, _ = interact_with_reward_model(\n",
    "            result_prompt, choices, true_answer, reward_model, reward_tokenizer, device\n",
    "        )\n",
    "        prompt_list.append(result_prompt)\n",
    "        llm_answer_list.append(llm_answer)\n",
    "        correct_answer_list.append(true_answer)\n",
    "\n",
    "    df_sampled = pd.DataFrame(\n",
    "        {\n",
    "            \"prompt\": prompt_list,\n",
    "            \"response\": llm_answer_list,\n",
    "            \"correct\": correct_answer_list,\n",
    "        }\n",
    "    )\n",
    "    if save_path is not None:\n",
    "        # `save_path`ê°€ ìˆìœ¼ë©´ íŒŒì¼ë¡œ ì €ì¥\n",
    "        if not save_path.endswith(\".csv\"):\n",
    "            save_path += \".csv\"\n",
    "        df_sampled.to_csv(save_path, index=True)\n",
    "    return df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sP9_Se1M5Mb"
   },
   "outputs": [],
   "source": [
    "sampled_result_path = join_path(SAMPLE_RESULT_CSV)\n",
    "df_sampled = sampled_result_as_dataframe(\n",
    "    df_train,\n",
    "    policy_model,\n",
    "    policy_tokenizer,\n",
    "    reward_model,\n",
    "    reward_tokenizer,\n",
    "    save_path=sampled_result_path,\n",
    "    num_sample=30,\n",
    "    device=device,\n",
    ")\n",
    "print(f\"ğŸ¤”ì €ì¥í•œ ê²°ê³¼ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”: {sampled_result_path}\")\n",
    "df_sampled.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
