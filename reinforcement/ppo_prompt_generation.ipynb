{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4DpTEsmfSZg"
   },
   "source": [
    "# Prompt generation with PPO algorithm\n",
    "\n",
    "- `PPO`: Proximal policy optimization\n",
    "- 강화학습 기반 시스템 프롬프트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzvSLLIpliOL"
   },
   "outputs": [],
   "source": [
    "# 🔥하이퍼파라미터 설정\n",
    "BASE_DIR = \"/content/drive/MyDrive/강화학습\"\n",
    "REWARD_MODEL_DIR = \"llama3\"\n",
    "POLICY_MODEL_DIR = \"exaone\"\n",
    "TRAINED_OUTPUT = \"ppo_exaone\"\n",
    "TRAIN_DATA = \"train.csv\"\n",
    "SAMPLE_RESULT_CSV = \"result_prompt.csv\"\n",
    "RANDOM_SEED = 42\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_ITER = 10\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_STEP = 1\n",
    "LOG_STEP = 100\n",
    "TOKENIZER_MAX_LENGTH = 1024\n",
    "POLICY_MAX_NEW_TOKENS = 1024\n",
    "POLICY_DO_SAMPLE = True\n",
    "POLICY_TEMPERATURE = 0.1\n",
    "REWARD_MAX_NEW_TOKENS = 32\n",
    "REWARD_DO_SAMPLE = False\n",
    "REWARD_TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2rqEE6FL2JP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CUDA 디버깅\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uat-01SqM5MY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU를 사용하세요!\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxpltCKB5lLZ"
   },
   "outputs": [],
   "source": [
    "!pip install -qq \\\n",
    "  fsspec==2025.3.2 \\\n",
    "  accelerate bitsandbytes \\\n",
    "  transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhVUTw7Y5y-J"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import ast\n",
    "import random\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import pandas as pd\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "\n",
    "def join_path(*args):\n",
    "    return os.path.join(BASE_DIR, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsAgtQlUM5MZ"
   },
   "outputs": [],
   "source": [
    "# CUDA 최적화\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if hasattr(torch.backends.cuda, \"matmul\") and hasattr(\n",
    "    torch.backends.cuda.matmul, \"allow_tf32\"\n",
    "):\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy_prompt() -> list:\n",
    "    # EXAONE에서 사용할 프롬프트\n",
    "    sys_prompt = \"당신은 LLM이 주어진 질문과 선택지를 감정 없이 중립적이고 논리적으로 판단할 수 있도록 유도하는 시스템 프롬프트를 작성하는 전문가입니다.\"\n",
    "    user_prompt = (\n",
    "        \"조건:\\n\"\n",
    "        \"- 프롬프트는 한 문장이어야 하며, 시스템 메시지로 사용될 수 있어야 합니다.\\n\"\n",
    "        \"- 편향, 감정 표현 없이 다양한 관점에서 사고를 유도해야 합니다.\\n\"\n",
    "        \"- [맥락], [질문], [선택지]가 사용자 입력으로 주어질 예정이므로, 이를 고려해 일반화된 형태로 작성하세요.\\n\"\n",
    "        \"- 마크다운 형식을 사용하지 마세요.\\n\"\n",
    "        \"- 프롬프트만 출력하고, 그 외 설명은 하지 마세요.\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_reward_prompt(instruction, context, question, choices) -> str:\n",
    "    # Llama3에서 사용할 프롬프트\n",
    "    choices_str = \"\\n\".join([f\"{i}. {choice}\" for i, choice in enumerate(choices, 1)])\n",
    "    return (\n",
    "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
    "        f\"{instruction}<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"맥락: {context.strip()}\\n\"\n",
    "        f\"질문: {question.strip()}\\n\"\n",
    "        \"선택지:\\n\"\n",
    "        f\"{choices_str}\\n\"\n",
    "        \"최종 답변은 설명 없이 1, 2, 3 중 하나로만 작성하시오.\\n\"\n",
    "        \"최종 답변:<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_last_choice(raw_answer, choices) -> str:\n",
    "    first_digit = next(\n",
    "        (char for char in raw_answer if char.isdigit()), None\n",
    "    )  # 처음 나오는 숫자만 추출\n",
    "    if first_digit.isdigit():\n",
    "        last_choice_idx = int(first_digit)\n",
    "        if 1 <= last_choice_idx <= 3:\n",
    "            last_choice = choices[last_choice_idx - 1]\n",
    "            return last_choice\n",
    "\n",
    "    raw_answer = raw_answer.strip().replace(\"\\n\", \"\")\n",
    "    return raw_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPHaTNlv711L"
   },
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "quat_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(join_path(POLICY_MODEL_DIR))\n",
    "if policy_tokenizer.pad_token_id is None:\n",
    "    policy_tokenizer.pad_token_id = policy_tokenizer.eos_token_id\n",
    "\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    join_path(POLICY_MODEL_DIR),\n",
    "    quantization_config=quat_config,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# meta-llama/Llama-3.1-8B-Instruct\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    join_path(REWARD_MODEL_DIR), padding_side=\"left\"\n",
    ")\n",
    "if reward_tokenizer.pad_token_id is None:\n",
    "    reward_tokenizer.pad_token_id = reward_tokenizer.eos_token_id\n",
    "\n",
    "reward_model = AutoModelForCausalLM.from_pretrained(\n",
    "    join_path(REWARD_MODEL_DIR),\n",
    "    quantization_config=quat_config,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAqm3domoKzh"
   },
   "outputs": [],
   "source": [
    "def generate_prompt_with_policy(messages, policy_model, policy_tokenizer, device):\n",
    "    input_ids = policy_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZER_MAX_LENGTH,\n",
    "    ).to(device)\n",
    "    response_id = policy_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=POLICY_MAX_NEW_TOKENS,\n",
    "        tokenizer=policy_tokenizer,\n",
    "        do_sample=POLICY_DO_SAMPLE,\n",
    "        temperature=POLICY_TEMPERATURE,\n",
    "        eos_token_id=policy_tokenizer.eos_token_id,\n",
    "        pad_token_id=policy_tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )[0]\n",
    "    action_result = (\n",
    "        policy_tokenizer.decode(response_id)\n",
    "        .split(\"[|assistant|]\")[-1]\n",
    "        .split(\"[|endofturn|]\")[0]\n",
    "        .strip()\n",
    "    )\n",
    "    return input_ids, response_id, action_result\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def interact_with_reward_model(\n",
    "    prompt, choices, true_answer, reward_model, reward_tokenizer, device\n",
    "):\n",
    "    input_ids = reward_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZER_MAX_LENGTH,\n",
    "    ).to(device)\n",
    "    response_id = reward_model.generate(\n",
    "        **input_ids,\n",
    "        max_new_tokens=REWARD_MAX_NEW_TOKENS,\n",
    "        do_sample=REWARD_DO_SAMPLE,\n",
    "        temperature=REWARD_DO_SAMPLE,\n",
    "        eos_token_id=reward_tokenizer.eos_token_id,\n",
    "        pad_token_id=reward_tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "        use_cache=True,\n",
    "    )[0]\n",
    "    result_state = (\n",
    "        reward_tokenizer.decode(response_id, skip_special_tokens=True)\n",
    "        .replace(prompt, \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    result_state = extract_last_choice(result_state, choices)\n",
    "    # 정답이면 reward = +1, 아니면 -1\n",
    "    reward = 1.0 if result_state == true_answer else -1.0\n",
    "    return result_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_set(row):\n",
    "    context = row[\"context\"].strip()\n",
    "    question = row[\"question\"].strip()\n",
    "    choices = ast.literal_eval(row[\"choices\"])\n",
    "    true_answer = row[\"answer\"].strip()\n",
    "    return context, question, choices, true_answer\n",
    "\n",
    "\n",
    "# 데이터 준비\n",
    "df_train = pd.read_csv(join_path(TRAIN_DATA), encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9r6IwdfM5Ma"
   },
   "outputs": [],
   "source": [
    "class PPOBatch:\n",
    "    def __init__(self, max_batch_size, device=\"cuda\"):\n",
    "        self.queries = []\n",
    "        self.responses = []\n",
    "        self.rewards = []\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def clear(self):\n",
    "        self.queries.clear()\n",
    "        self.responses.clear()\n",
    "        self.rewards.clear()\n",
    "\n",
    "    def append(self, query, response, reward):\n",
    "        assert len(self) <= self.max_batch_size, \"Batch size limit exceeded.\"\n",
    "        self.queries.append(query.squeeze(0))\n",
    "        self.responses.append(response.squeeze(0))\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def get_all(self):\n",
    "        reward_tensors = [\n",
    "            torch.tensor(reward, dtype=torch.float32).to(self.device)\n",
    "            for reward in self.rewards\n",
    "        ]\n",
    "        return self.queries, self.responses, reward_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gs3Ku2XNS_w"
   },
   "outputs": [],
   "source": [
    "# PPO 학습 설정\n",
    "# PPOConfig와 PPOTrainer가 deprecated 버전임을 알고 있지만\n",
    "# v2는 사용하는 모델과 충돌이 있어 v1을 사용합니다.\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=policy_model,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=GRADIENT_STEP,\n",
    "    output_dir=join_path(\"checkpoint\"),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=LOG_STEP,\n",
    "    save_total_limit=3,\n",
    "    log_with=None,\n",
    ")\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    tokenizer=policy_tokenizer,\n",
    "    dataset=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZBy54_V899a"
   },
   "outputs": [],
   "source": [
    "os.makedirs(join_path(\"checkpoint\"), exist_ok=True)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# PPO 학습\n",
    "total_reward = 0\n",
    "num_total_reward = 0\n",
    "\n",
    "for epoch in range(1, MAX_ITER + 1):\n",
    "    ppo_batch = PPOBatch(BATCH_SIZE, device)\n",
    "\n",
    "    for idx, row in df_train.iterrows():\n",
    "        context, question, choices, true_answer = get_train_set(row)\n",
    "\n",
    "        # Policy를 통해 프롬프트(action) 생성\n",
    "        train_prompt = generate_policy_prompt()\n",
    "        input_tokens, result_prompt_tokens, result_prompt = generate_prompt_with_policy(\n",
    "            train_prompt, policy_model, policy_tokenizer, device\n",
    "        )\n",
    "        # print(\"===== Action =====\\n\", result_prompt)\n",
    "\n",
    "        # Action를 통해 reward 생성\n",
    "        result_prompt = generate_reward_prompt(\n",
    "            result_prompt, context, question, choices\n",
    "        )\n",
    "        llm_answer, reward = interact_with_reward_model(\n",
    "            result_prompt, choices, true_answer, reward_model, reward_tokenizer, device\n",
    "        )\n",
    "        # print(\"===== State =====\\n\", llm_answer)\n",
    "        # print(\"===== Reward =====\\n\", reward, \"\\n\")\n",
    "\n",
    "        # 정보 기록\n",
    "        total_reward += reward\n",
    "        num_total_reward += 1\n",
    "        ppo_batch.append(input_tokens, result_prompt_tokens, reward)\n",
    "\n",
    "        if len(ppo_batch) == BATCH_SIZE:\n",
    "            # 파라미터 학습\n",
    "            queries, responses, rewards = ppo_batch.get_all()\n",
    "            ppo_trainer.step(queries, responses, rewards)\n",
    "            ppo_batch.clear()\n",
    "\n",
    "        if idx % LOG_STEP == 0:\n",
    "            # 학습 현황 출력\n",
    "            avg_reward = total_reward / num_total_reward\n",
    "            print(f\"[{epoch}_{idx}] Average reward: {avg_reward:.3f}\")\n",
    "\n",
    "            total_reward = 0\n",
    "            num_total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCgWLg3OM5Mb"
   },
   "outputs": [],
   "source": [
    "ppo_trainer.model.save_pretrained(join_path(TRAINED_OUTPUT))\n",
    "policy_tokenizer.save_pretrained(join_path(TRAINED_OUTPUT))\n",
    "print(\"🫠학습을 완료했습니다!\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y3MY6ScM5Mb"
   },
   "outputs": [],
   "source": [
    "def random_sample(df):\n",
    "    radom_idx = random.randint(0, len(df) - 1)\n",
    "    row = df.iloc[radom_idx]\n",
    "    context = row[\"context\"].strip()\n",
    "    question = row[\"question\"].strip()\n",
    "    choices = ast.literal_eval(row[\"choices\"])\n",
    "    true_answer = row[\"answer\"].strip()\n",
    "    return context, question, choices, true_answer\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sampled_result_as_dataframe(\n",
    "    df,\n",
    "    policy_model,\n",
    "    policy_tokenizer,\n",
    "    reward_model,\n",
    "    reward_tokenizer,\n",
    "    save_path=None,\n",
    "    num_sample=10,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    prompt_list = []\n",
    "    llm_answer_list = []\n",
    "    correct_answer_list = []\n",
    "\n",
    "    for i in range(num_sample):\n",
    "        context, question, choices, true_answer = random_sample(df)\n",
    "        train_prompt = generate_policy_prompt()\n",
    "        _, _, result_prompt = generate_prompt_with_policy(\n",
    "            train_prompt, policy_model, policy_tokenizer, device\n",
    "        )\n",
    "        result_prompt = generate_reward_prompt(\n",
    "            result_prompt, context, question, choices\n",
    "        )\n",
    "        llm_answer, _ = interact_with_reward_model(\n",
    "            result_prompt, choices, true_answer, reward_model, reward_tokenizer, device\n",
    "        )\n",
    "        prompt_list.append(result_prompt)\n",
    "        llm_answer_list.append(llm_answer)\n",
    "        correct_answer_list.append(true_answer)\n",
    "\n",
    "    df_sampled = pd.DataFrame(\n",
    "        {\n",
    "            \"prompt\": prompt_list,\n",
    "            \"response\": llm_answer_list,\n",
    "            \"correct\": correct_answer_list,\n",
    "        }\n",
    "    )\n",
    "    if save_path is not None:\n",
    "        # `save_path`가 있으면 파일로 저장\n",
    "        if not save_path.endswith(\".csv\"):\n",
    "            save_path += \".csv\"\n",
    "        df_sampled.to_csv(save_path, index=True)\n",
    "    return df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sP9_Se1M5Mb"
   },
   "outputs": [],
   "source": [
    "sampled_result_path = join_path(SAMPLE_RESULT_CSV)\n",
    "df_sampled = sampled_result_as_dataframe(\n",
    "    df_train,\n",
    "    policy_model,\n",
    "    policy_tokenizer,\n",
    "    reward_model,\n",
    "    reward_tokenizer,\n",
    "    save_path=sampled_result_path,\n",
    "    num_sample=30,\n",
    "    device=device,\n",
    ")\n",
    "print(f\"🤔저장한 결과를 확인해 보세요: {sampled_result_path}\")\n",
    "df_sampled.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
