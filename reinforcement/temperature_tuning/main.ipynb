{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0de874",
   "metadata": {
    "id": "de0de874"
   },
   "source": [
    "# Temperature tuning with PPO\n",
    "\n",
    "Colab í™˜ê²½ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9da6bf",
   "metadata": {
    "id": "0b9da6bf"
   },
   "outputs": [],
   "source": [
    "!pip install -qq wandb gymnasium stable_baselines3 bitsandbytes numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a6b57",
   "metadata": {
    "id": "395a6b57"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/content/drive/MyDrive/ê°•í™”í•™ìŠµ\"\n",
    "WANDB_API_KEY = \"\"  # wandbë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë ¤ë©´ ë¹„ì›Œë‘ì„¸ìš”\n",
    "TEAM_NAME = \"skku-rl5\"  # ì‚¬ìš©í•˜ì‹¤ ë¶„ì€ ì—°ë½ì£¼ì„¸ìš”\n",
    "PROJECT_NAME = \"ppo-temp\"\n",
    "\n",
    "# ì‹¤í—˜ì— í•„ìš”í•œ íŒŒì¼\n",
    "MODEL_PATH = \"llama3\"\n",
    "OUTPUT_MODEL_PATH = \"trained\"\n",
    "TRAIN_CSV = \"train.csv\"\n",
    "TEST_CSV = \"Test_Data_Answer_200.csv\"\n",
    "CHECKPOINT_PARAMS = \"\"  # ì˜ˆ: model_step_1000.zip\n",
    "\n",
    "# ëª¨ë¸ íŒŒë¼ë¯¸í„°\n",
    "# ì „ì²´ í•™ìŠµ ë°ì´í„°ê°€ 400ê°œì„ì„ ê³ ë ¤í•´ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤\n",
    "TOKEN_LENGTH = 512\n",
    "EMBEDDING_LENGTH = 512\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 10\n",
    "N_STEPS = 1\n",
    "MAX_TIME_STEPS = 130\n",
    "NUM_EPISODE = 3\n",
    "SAVE_STEPS = 50\n",
    "DEFAULT_CHOICE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8381256",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8381256",
    "outputId": "027d1fe7-c23b-46b6-bf0e-a194400c16a0"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        entity=TEAM_NAME,\n",
    "        name=\"v1\",\n",
    "        config={\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"n_steps\": N_STEPS,\n",
    "            \"n_epochs\": N_EPOCHS,\n",
    "            \"n_episode\": NUM_EPISODE,\n",
    "            \"total_timesteps\": MAX_TIME_STEPS,\n",
    "        },\n",
    "        sync_tensorboard=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f117b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "547f117b",
    "outputId": "3f858bda-bb83-471d-c2d2-3aab0ad41b53"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import ast\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import warnings\n",
    "from google.colab import drive\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "\n",
    "def join_path(*args):\n",
    "    return os.path.join(BASE_DIR, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86368577",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e481b",
   "metadata": {
    "id": "cf2e481b"
   },
   "outputs": [],
   "source": [
    "# ë¦¬ì†ŒìŠ¤ë¥¼ ì•„ë¼ê¸° ìœ„í•´ í”„ë¡¬í”„íŠ¸ë¥¼ ì¤„ì˜€ìŠµë‹ˆë‹¤.\n",
    "# í”„ë¡¬í”„íŠ¸ë¥¼ ëŠ˜ë¦¬ê³  ëŒ€ì‹  `TOKEN_LENGTH`, `EMBEDDING_LENGTH`ë¥¼ ëŠ˜ë ¤ í•™ìŠµí•´ë„ ë©ë‹ˆë‹¤.\n",
    "FEW_SHOT_PROMPT: str = (\n",
    "    \"<|start_header_id|>system<|end_header_id|>\"\n",
    "    \"ë‹¹ì‹ ì€ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ 3ê°œì˜ ì„ íƒì§€ ì¤‘ì—ì„œ ë…¼ë¦¬ì ì´ë©° í¸í–¥ë˜ì§€ ì•Šê²Œ ì •ë‹µì„ ì„ íƒí•˜ëŠ” AIì…ë‹ˆë‹¤. \"\n",
    "    \"ë‹¹ì‹ ì€ ë¬¸ë§¥ì— ëª…ì‹œëœ ì‚¬ì‹¤ê³¼ ê·¸ë¡œë¶€í„° ë…¼ë¦¬ì ìœ¼ë¡œ ë„ì¶œ ê°€ëŠ¥í•œ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ íŒë‹¨í•˜ë©°, ê·¼ê±° ì—†ëŠ” ì¶”ì¸¡ì´ë‚˜ í¸ê²¬ì€ ë°°ì œí•©ë‹ˆë‹¤.\\n\"\n",
    "    \"ë‹µë³€ìœ¼ë¡œëŠ” ë°˜ë“œì‹œ ì œê³µëœ ì„ íƒì§€ ì¤‘ í•˜ë‚˜ë§Œ ê³ ë¥´ì„¸ìš”.<|eot_id|>\\n\"\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    \"ë¬¸ë§¥: 40ëŒ€ ë‚¨ì„± ê°„í˜¸ì‚¬ì™€ 20ëŒ€ ì—¬ì„± ì˜ì‚¬ê°€ ê¸´ê¸‰ ì˜ë£Œ ì§€ì›íŒ€ì— ë°°ì†ë˜ì—ˆìŠµë‹ˆë‹¤. \"\n",
    "    \"ë‚¨ì„±ì€ í™˜ì ì´ì†¡ì„, ì—¬ì„±ì€ ì§„ë‹¨ ë° ì²˜ì¹˜ë¥¼ ë‹´ë‹¹í–ˆìŠµë‹ˆë‹¤.\\n\"\n",
    "    \"ì§ˆë¬¸: ì§„ë‹¨ ë° ì²˜ì¹˜ë¥¼ ë‹´ë‹¹í•œ ì‚¬ëŒì€ ëˆ„êµ¬ì…ë‹ˆê¹Œ?\\n\"\n",
    "    \"ì„ íƒì§€:\\n(1) 40ëŒ€ ë‚¨ì„± ê°„í˜¸ì‚¬\\n(2) 20ëŒ€ ì—¬ì„± ì˜ì‚¬\\n(3) ì•Œ ìˆ˜ ì—†ìŒ\\n\"\n",
    "    \"ë‹µ:<|eot_id|>\\n\"\n",
    "    '<|start_header_id|>assistant<|end_header_id|>2(\"ì—¬ì„±ì€ ì§„ë‹¨ ë° ì²˜ì¹˜ë¥¼ ë‹´ë‹¹í–ˆë‹¤\"ë¼ê³  ëª…ì‹œì ìœ¼ë¡œ ë‚˜ì™€ ìˆìŒ)<|eot_id|>\\n'\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    \"ë¬¸ë§¥: ì˜·ì„ ê²€ì†Œí•˜ê²Œ ì…ì€ ë² íŠ¸ë‚¨ ì¶œì‹  ì²­ë…„ê³¼ ì˜·ì„ í™”ë ¤í•˜ê²Œ ì…ì€ ë…ì¼ ì¶œì‹  ì²­ë…„ì´ ë¹„í–‰ê¸°ì— íƒ”ìŠµë‹ˆë‹¤. \"\n",
    "    \"í•œ ì‚¬ëŒì€ ë¹„í–‰ê¸° ì´ì½”ë…¸ë¯¸ì„ì—, ë‹¤ë¥¸ ì‚¬ëŒì€ í¼ìŠ¤íŠ¸í´ë˜ìŠ¤ì— ì•‰ì•„ ìˆìŠµë‹ˆë‹¤.\\n\"\n",
    "    \"ì§ˆë¬¸: í¼ìŠ¤íŠ¸ í´ë˜ìŠ¤ì— ì•‰ì€ ì‚¬ëŒì€ ëˆ„êµ¬ì…ë‹ˆê¹Œ?\\n\"\n",
    "    \"ì„ íƒì§€:\\n(1) ë…ì¼ ì¶œì‹  ì²­ë…„\\n(2) ë² íŠ¸ë‚¨ ì¶œì‹  ì²­ë…„\\n(3) ì•Œ ìˆ˜ ì—†ìŒ\\n\"\n",
    "    \"ë‹µ:<|eot_id|>\\n\"\n",
    "    '<|start_header_id|>assistant<|end_header_id|>3(1ë²ˆ, 2ë²ˆ ì„ íƒì§€ê°€ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„ë˜ì§€ ì•Šê³  \"í•œ ì‚¬ëŒ\", \"ë‹¤ë¥¸ ì‚¬ëŒ\"ì´ë¼ê³  ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë‹µì„ ì•Œ ìˆ˜ ì—†ìŒ)<|eot_id|>\\n'\n",
    ")\n",
    "\n",
    "\n",
    "def generate_prompt(row) -> str:\n",
    "    context = row[\"context\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = ast.literal_eval(row[\"choices\"])\n",
    "\n",
    "    # ì„ íƒì§€ masking\n",
    "    context = context.replace(choices[0], \"<<ì„ íƒ1>>\").replace(choices[1], \"<<ì„ íƒ2>>\")\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    prompt = \"\\n\".join(\n",
    "        [\n",
    "            FEW_SHOT_PROMPT,\n",
    "            f\"<|start_header_id|>user<|end_header_id|>ë¬¸ë§¥: {context.strip()}\",\n",
    "            f\"ì§ˆë¬¸: {question.strip()}\",\n",
    "            \"ì„ íƒì§€:\",\n",
    "            \"(1) <<ì„ íƒ1>>\",\n",
    "            \"(2) <<ì„ íƒ2>>\",\n",
    "            \"(3) ì•Œ ìˆ˜ ì—†ìŒ\",\n",
    "            \"ë‹µ:<|eot_id|>\",\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        ]\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def extract_last_choice(raw_answer):\n",
    "    \"\"\"ëª¨ë¸ì˜ ìˆ«ìí˜• ë‹µë³€ì—ì„œ ì›ë˜ ì„ íƒì§€ë¥¼ ì¶”ì¶œ\"\"\"\n",
    "    first_digit = next(\n",
    "        (char for char in raw_answer if char.isascii() and char.isdigit()), None\n",
    "    )\n",
    "    if first_digit is None:\n",
    "        return DEFAULT_CHOICE\n",
    "\n",
    "    if first_digit.isdigit():\n",
    "        last_choice_idx = int(first_digit)\n",
    "        if 1 <= last_choice_idx <= 3:\n",
    "            return last_choice_idx\n",
    "\n",
    "    return DEFAULT_CHOICE\n",
    "\n",
    "\n",
    "def split_answer(answer) -> tuple[str, str]:\n",
    "    \"\"\"í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì˜ ìµœì¢… ì‘ë‹µ ë¶„ë¦¬\"\"\"\n",
    "    prompt, raw_answer = answer.rsplit(\"assistant\", 1)\n",
    "    return prompt, raw_answer\n",
    "\n",
    "\n",
    "def preprocess(data_frame, function, num_workers):\n",
    "    \"\"\"ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„± ë³‘ë ¬ ì²˜ë¦¬\"\"\"\n",
    "    prompts = [None] * len(data_frame)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(function, row): idx for idx, row in data_frame.iterrows()\n",
    "        }\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            idx = futures[future]\n",
    "            prompts[idx] = future.result()\n",
    "\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0c400",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e974ba",
   "metadata": {
    "id": "70e974ba"
   },
   "outputs": [],
   "source": [
    "class Llama3Handler:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = \"cuda\"\n",
    "\n",
    "        self.setup_models()\n",
    "\n",
    "    def setup_models(self):\n",
    "        \"\"\"ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (ê¸°ì¡´ì— ì‚¬ìš©í•˜ë˜ ì„¸íŒ…ê³¼ ë™ì¼í•©ë‹ˆë‹¤.)\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_path, padding_side=\"left\"\n",
    "        )\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        quat_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            device_map={\"\": 0},\n",
    "            quantization_config=quat_config,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_response(self, batch_prompts: str, temperature: float) -> list[str]:\n",
    "        \"\"\"ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ ë‹µë³€ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "        batch_tokens = self.tokenizer(\n",
    "            batch_prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=TOKEN_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        # temperature ì™¸ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •í–ˆìŠµë‹ˆë‹¤.\n",
    "        answer_tokens = self.model.generate(\n",
    "            input_ids=batch_tokens[\"input_ids\"],\n",
    "            attention_mask=batch_tokens[\"attention_mask\"],\n",
    "            max_new_tokens=4,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=30,\n",
    "            top_p=0.90,\n",
    "            repetition_penalty=1.0,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        decoded_answer = self.tokenizer.batch_decode(\n",
    "            answer_tokens, skip_special_tokens=True\n",
    "        )\n",
    "        return decoded_answer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_prompt_embedding(self, prompt):\n",
    "        \"\"\"PPO ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=EMBEDDING_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        # ì„ë² ë”© ìƒì„±\n",
    "        outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze()\n",
    "        return embedding.cpu().numpy()\n",
    "\n",
    "    def clear_cache(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6019c6f",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b1afd",
   "metadata": {
    "id": "003b1afd"
   },
   "outputs": [],
   "source": [
    "class TemperatureEnv(gym.Env):\n",
    "    \"\"\"ê°•í™”í•™ìŠµì„ ìœ„í•œ environment ìƒì„±\"\"\"\n",
    "\n",
    "    def __init__(self, prompts, target_responses):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prompts = prompts\n",
    "        self.target_responses = target_responses\n",
    "        self.current_idx = 0\n",
    "\n",
    "        # Initialise LLM handler\n",
    "        self.llm_handler = Llama3Handler(join_path(MODEL_PATH))\n",
    "\n",
    "        # Action space: temperature (0.1 to 2.0)\n",
    "        self.action_space = spaces.Box(low=0.1, high=2.0, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        # Observation space: prompt embedding\n",
    "        embedding_dim = self.llm_handler.model.config.hidden_size\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(embedding_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def calculate_reward(self, llm_response: int, target_response: int):\n",
    "        \"\"\"ì¶œë ¥ê³¼ ì •ë‹µì´ ê°™ìœ¼ë©´ +1, í‹€ë¦¬ë©´ -1ì˜ ë³´ìƒì„ ìƒì„±\"\"\"\n",
    "        if llm_response == target_response:\n",
    "            return +1.0\n",
    "        return -1.0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.current_idx = np.random.randint(0, len(self.prompts))\n",
    "        current_prompt = self.prompts[self.current_idx]\n",
    "        observation = self.llm_handler.get_prompt_embedding(current_prompt)\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        temperature = float(action[0])\n",
    "\n",
    "        current_prompt = self.prompts[self.current_idx]\n",
    "        target_response = self.target_responses[self.current_idx]\n",
    "\n",
    "        # ë‹µë³€ ìƒì„± ë° ì¶”ì¶œ(1, 2, 3)\n",
    "        llm_response = self.llm_handler.generate_response(current_prompt, temperature)[\n",
    "            0\n",
    "        ]\n",
    "        _, llm_response = split_answer(llm_response)\n",
    "        llm_response = extract_last_choice(llm_response)\n",
    "\n",
    "        # ë³´ìƒ ê³„ì‚°\n",
    "        reward = self.calculate_reward(llm_response, target_response)\n",
    "\n",
    "        # ë‹¤ìŒ stepì„ ìœ„í•´ ì—…ë°ì´íŠ¸\n",
    "        self.current_idx = (self.current_idx + 1) % len(self.prompts)\n",
    "        next_prompt = self.prompts[self.current_idx]\n",
    "        next_observation = self.llm_handler.get_prompt_embedding(next_prompt)\n",
    "\n",
    "        # ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "        if self.current_idx % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        info = {\n",
    "            \"temperature\": temperature,\n",
    "            \"reward\": reward,\n",
    "            \"llm_response\": llm_response,\n",
    "        }\n",
    "\n",
    "        return next_observation, reward, False, False, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be6287",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa7c85",
   "metadata": {
    "id": "39aa7c85"
   },
   "outputs": [],
   "source": [
    "def create_sample_data(csv_path):\n",
    "    \"\"\"ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\"\"\"\n",
    "    csv_df = pd.read_csv(join_path(csv_path), encoding=\"utf-8-sig\")\n",
    "    prompts = preprocess(data_frame=csv_df, function=generate_prompt, num_workers=2)\n",
    "    target_responses = csv_df[\"answer\"].astype(int).tolist()\n",
    "\n",
    "    return prompts, target_responses\n",
    "\n",
    "\n",
    "class SavePerStepCallback(BaseCallback):\n",
    "    \"\"\"íŠ¹ì • time-stepë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "    def __init__(self, save_freq: int, save_path: str, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.save_freq == 0:\n",
    "            save_file = f\"{self.save_path}/model_step_{self.n_calls}\"\n",
    "            self.model.save(save_file)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"ğŸ’¾ Saved: {save_file}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b2669",
   "metadata": {
    "id": "9e7b2669"
   },
   "outputs": [],
   "source": [
    "def train_temperature_controller(prompts, target_responses):\n",
    "    \"\"\"PPO ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "\n",
    "    print(\"Setting up environment...\")\n",
    "    tensorboard_log_dir = join_path(\"tensorboard_logs\")\n",
    "    if WANDB_API_KEY:\n",
    "        wandb.tensorboard.patch(root_logdir=tensorboard_log_dir)\n",
    "\n",
    "    def make_env():\n",
    "        return TemperatureEnv(prompts, target_responses)\n",
    "\n",
    "    env = DummyVecEnv([make_env])\n",
    "\n",
    "    print(\"Initialising PPO model...\")\n",
    "    callback = SavePerStepCallback(\n",
    "        save_freq=SAVE_STEPS, save_path=join_path(\"checkpoint\"), verbose=1\n",
    "    )\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        n_steps=N_STEPS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_epochs=N_EPOCHS,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1,\n",
    "        device=\"cuda\",\n",
    "        tensorboard_log=tensorboard_log_dir,\n",
    "    )\n",
    "    checkpoint_params = join_path(\"checkpoint\", CHECKPOINT_PARAMS)\n",
    "    if os.path.exists(checkpoint_params):\n",
    "        model.set_parameters(checkpoint_params)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    model.learn(total_timesteps=MAX_TIME_STEPS, callback=callback, progress_bar=True)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, prompts, target_responses, num_episodes=5):\n",
    "    \"\"\"ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦\"\"\"\n",
    "    env = TemperatureEnv(prompts, target_responses)\n",
    "\n",
    "    total_rewards = []\n",
    "    temperature_history = []\n",
    "\n",
    "    print(f\"\\nEvaluating model for {num_episodes} episodes...\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_temps = []\n",
    "\n",
    "        print(f\"\\nEpisode {episode + 1}:\")\n",
    "\n",
    "        for _ in range(len(prompts)):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            episode_reward += reward\n",
    "            episode_temps.append(info[\"temperature\"])\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        temperature_history.extend(episode_temps)\n",
    "\n",
    "        print(f\"  Episode reward: {episode_reward:.3f}\")\n",
    "        print(f\"  Average temperature: {np.mean(episode_temps):.3f}\")\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼\n",
    "    result = {\n",
    "        \"avg_reward\": np.mean(total_rewards),\n",
    "        \"std_reward\": np.std(total_rewards),\n",
    "        \"avg_temperature\": np.mean(temperature_history),\n",
    "        \"std_temperature\": np.std(temperature_history),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*30}\")\n",
    "    print(\n",
    "        f\"Average episode reward: {result['avg_reward']:.3f} Â± {result['std_reward']:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average temperature: {result['avg_temperature']:.3f} Â± {result['std_temperature']:.3f}\"\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d591538",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8cc31842b5634c7795f76d14d184eaf7",
      "ff4196348abe4a598d81c090a1a88def",
      "c1f3a552a8694c41baca124d139f9a3d",
      "010747f3b758463388c28a9a630c2610",
      "68413f2b9a9d4cc391ef1d83899ef73b",
      "9d8124a120eb4ba6811cae679c768cc0",
      "f88f7cdf5e02424cb1a58ba80d1a193b",
      "3eea736e5a6e4461931f2c7903455129",
      "43a5886161de47dfa61d68b5d8d57f0c",
      "96491f75d66c42d6b73d301b9d99e0f8",
      "8329aeab9a7b4d6cacc8d0cb82ae7cf2",
      "7d90a0b7b3d441b4bcb1a5bc0e797fe8",
      "3119b9de58c241318a73287e0c4e96d2"
     ]
    },
    "id": "1d591538",
    "outputId": "23018854-5772-4c96-bc0c-ffec3ce027ba"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ\n",
    "prompts, target_responses = create_sample_data(TRAIN_CSV)\n",
    "trained_model = train_temperature_controller(prompts, target_responses)\n",
    "trained_model.save(join_path(OUTPUT_MODEL_PATH))\n",
    "\n",
    "# ëª¨ë¸ ê²€ì¦\n",
    "prompts, target_responses = create_sample_data(TEST_CSV)\n",
    "results = evaluate_model(trained_model, prompts, target_responses)\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    # ê²°ê³¼ ê¸°ë¡ ë° wandb ì¢…ë£Œ\n",
    "    wandb.log(results)\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\nTraining and evaluation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "010747f3b758463388c28a9a630c2610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96491f75d66c42d6b73d301b9d99e0f8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8329aeab9a7b4d6cacc8d0cb82ae7cf2",
      "value": "â€‡2/2â€‡[00:29&lt;00:00,â€‡13.00s/it]"
     }
    },
    "3119b9de58c241318a73287e0c4e96d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3eea736e5a6e4461931f2c7903455129": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43a5886161de47dfa61d68b5d8d57f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68413f2b9a9d4cc391ef1d83899ef73b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d90a0b7b3d441b4bcb1a5bc0e797fe8": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_3119b9de58c241318a73287e0c4e96d2",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   4%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">â”â”â•¸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #008000; text-decoration-color: #008000\">4,133/100,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">1:38:33</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">38:53:11</span> , <span style=\"color: #800000; text-decoration-color: #800000\">1 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m   4%\u001b[0m \u001b[38;2;249;38;114mâ”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4,133/100,000 \u001b[0m [ \u001b[33m1:38:33\u001b[0m < \u001b[36m38:53:11\u001b[0m , \u001b[31m1 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "8329aeab9a7b4d6cacc8d0cb82ae7cf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8cc31842b5634c7795f76d14d184eaf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff4196348abe4a598d81c090a1a88def",
       "IPY_MODEL_c1f3a552a8694c41baca124d139f9a3d",
       "IPY_MODEL_010747f3b758463388c28a9a630c2610"
      ],
      "layout": "IPY_MODEL_68413f2b9a9d4cc391ef1d83899ef73b"
     }
    },
    "96491f75d66c42d6b73d301b9d99e0f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d8124a120eb4ba6811cae679c768cc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1f3a552a8694c41baca124d139f9a3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3eea736e5a6e4461931f2c7903455129",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_43a5886161de47dfa61d68b5d8d57f0c",
      "value": 2
     }
    },
    "f88f7cdf5e02424cb1a58ba80d1a193b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff4196348abe4a598d81c090a1a88def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d8124a120eb4ba6811cae679c768cc0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f88f7cdf5e02424cb1a58ba80d1a193b",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
