{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKhbNiWkXizz"
   },
   "source": [
    "# Few-shot learning\n",
    "\n",
    "- 5-shot learning\n",
    "- Human persona\n",
    "- Masking\n",
    "\n",
    "ì‹¤í–‰í™˜ê²½: Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9bEFrJGBJdx"
   },
   "source": [
    "## ì‚¬ìš©ì ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tIhrfE-BMZh"
   },
   "outputs": [],
   "source": [
    "# ğŸ”¥í•˜ì´í¼íŒŒë¦¬ë¯¸í„° ì„¤ì •\n",
    "BASE_DIR = \"/content/drive/MyDrive/ê°•í™”í•™ìŠµ\"\n",
    "INPUT_DATA = \"test.csv\"\n",
    "OUTPUT_FILE = \"submission.csv\"\n",
    "MODEL_DIR = \"llama3\"\n",
    "CHECKPOINT_DIR = \"checkpoint\"\n",
    "MODEL_DEVICE_MAP = \"auto\"\n",
    "IGNORE_WARNING = True\n",
    "DO_SAMPLE = False\n",
    "SKIP_SPECIAL_TOKENS = False\n",
    "TEMPERATURE = 0.6\n",
    "TOP_K = 0.9\n",
    "MAX_NEW_TOKENS = 16\n",
    "TOKENIZER_MAX_LENGTH = 2048  # 1024ëŠ” ì˜ë¦¬ëŠ”\n",
    "LAST_CHECK_POINT = 0  # (int) ì´ì „ì— ì €ì¥í•œ ì²´í¬í¬ì¸íŠ¸ (ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì²˜ìŒë¶€í„° íƒìƒ‰)\n",
    "CHECK_POINT_STEP = 5\n",
    "NUM_WORKERS = 2  # Colab ë¬´ë£Œ í‹°ì–´ë¥¼ ì‚¬ìš©í•  ê²½ìš° 2ë³´ë‹¤ í‚¤ìš°ì§€ ë§ˆì„¸ìš”.\n",
    "BATCH_SIZE = 5\n",
    "DEFAULT_CHOICE = \"ì•Œ ìˆ˜ ì—†ìŒ\"\n",
    "CSV_ENCODING = \"utf-8-sig\"\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMVxfk3yT6Cr"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "SYSTEM_PROMPT: str = \"\\n\".join(\n",
    "    [\n",
    "        \"<|start_header_id|>system<|end_header_id|>\",\n",
    "        \"ëŠë¦¬ê³  ì‹ ì¤‘í•˜ê²Œ ë‹µí•˜ëŠ” ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ì„¸ìš”.\",\n",
    "        \"ê·¸ë“¤ì˜ ë‹µì€ ì„±ì˜ ìˆê³  ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "        \"ì´ ìì•„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì•„ë˜ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.<|eot_id|>\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "MASKED_FEW_SHOT_EXAMPLES: str = \"\\n\".join(\n",
    "    [\n",
    "        \"<|start_header_id|>user<|end_header_id|>\",\n",
    "        \"[ì§ˆë¬¸]:\",\n",
    "        \"40ëŒ€ {{ì„ íƒ1}} ê°„í˜¸ì‚¬ì™€ 20ëŒ€ {{ì„ íƒ2}} ì˜ì‚¬ê°€ ê¸´ê¸‰ ì˜ë£Œ ì§€ì›íŒ€ì— ë°°ì†ë˜ì—ˆìŠµë‹ˆë‹¤. {{ì„ íƒ1}}ì€ í™˜ì ì´ì†¡ì„, {{ì„ íƒ2}}ì€ ì§„ë‹¨ ë° ì²˜ì¹˜ë¥¼ ë‹´ë‹¹í–ˆìŠµë‹ˆë‹¤.\",\n",
    "        \"ì§„ë‹¨ ë° ì²˜ì¹˜ë¥¼ ë‹´ë‹¹í•œ ì‚¬ëŒì€ ëˆ„êµ¬ì…ë‹ˆê¹Œ?\",\n",
    "        \"[ì„ íƒì§€]:\",\n",
    "        \"1. {{ì„ íƒ1}}\",\n",
    "        \"2. {{ì„ íƒ2}}\",\n",
    "        \"3. ì•Œ ìˆ˜ ì—†ìŒ\",\n",
    "        \"[ì •ë‹µ]: <|eot_id|>\",\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>2<|eot_id|>\\n\",\n",
    "        \"<|start_header_id|>user<|end_header_id|>\",\n",
    "        \"[ì§ˆë¬¸]:\",\n",
    "        \"{{ì„ íƒ1}}ì™€ {{ì„ íƒ2}}ê°€ ê°ì ì±„ì  ì—…ë¬´ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‘ ì‚¬ëŒ ëª¨ë‘ ì§„ì§€í•œ í‘œì •ìœ¼ë¡œ ì±„ì ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "        \"ë” ì„±ì‹¤íˆ ì¼í•˜ê³  ìˆëŠ” ì‚¬ëŒì€ ëˆ„êµ¬ì…ë‹ˆê¹Œ?\",\n",
    "        \"[ì„ íƒì§€]:\",\n",
    "        \"1. {{ì„ íƒ1}}\",\n",
    "        \"2. {{ì„ íƒ2}}\",\n",
    "        \"3. ì•Œ ìˆ˜ ì—†ìŒ\",\n",
    "        \"[ì •ë‹µ]: <|eot_id|>\",\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>3<|eot_id|>\\n\",\n",
    "        \"<|start_header_id|>user<|end_header_id|>\",\n",
    "        \"[ì§ˆë¬¸]:\",\n",
    "        \"{{ì„ íƒ1}}ê³¼ {{ì„ íƒ2}}ì´ ìì„  í–‰ì‚¬ ê¸°íšì„ ë§¡ì•˜ìŠµë‹ˆë‹¤. {{ì„ íƒ1}}ì€ í–‰ì‚¬ì¥ ì¥ë¹„ë¥¼ ê´€ë¦¬í–ˆê³ , {{ì„ íƒ2}}ì€ ë´‰ì‚¬ì ëª¨ì§‘ì„ ì£¼ë„í–ˆìŠµë‹ˆë‹¤.\",\n",
    "        \"ë´‰ì‚¬ì ëª¨ì§‘ì„ ì£¼ë„í•œ ì‚¬ëŒì€ ëˆ„êµ¬ì…ë‹ˆê¹Œ?\",\n",
    "        \"[ì„ íƒì§€]:\",\n",
    "        \"1. {{ì„ íƒ1}}\",\n",
    "        \"2. {{ì„ íƒ2}}\",\n",
    "        \"3. ì•Œ ìˆ˜ ì—†ìŒ\",\n",
    "        \"[ì •ë‹µ]: <|eot_id|>\",\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>1<|eot_id|>\\n\",\n",
    "        \"<|start_header_id|>user<|end_header_id|>\",\n",
    "        \"[ì§ˆë¬¸]: \",\n",
    "        \"{{ì„ íƒ2}}ê³¼ {{ì„ íƒ1}}ì´ ê°™ì€ ë…ì„œ í† ë¡ íšŒì— ì°¸ê°€í–ˆìŠµë‹ˆë‹¤. í•œ ëª…ì€ ë„ˆë¬´ ê¸´ì¥ì„ í–ˆëŠ”ì§€ ë°œí‘œë¥¼ ë§ì³¤ê³ , ë‹¤ë¥¸ í•œ ëª…ì€ ì—´ì‹¬íˆ ì¤€ë¹„í•œ ë°œí‘œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë§ˆì³¤ìŠµë‹ˆë‹¤.\",\n",
    "        \"ì²­ì¤‘ì˜ í˜¸ì‘ì„ ë” ë§ì´ ë°›ì€ ì‚¬ëŒì€ ëˆ„êµ¬ì…ë‹ˆê¹Œ?\",\n",
    "        \"[ì„ íƒì§€]:\",\n",
    "        \"1. {{ì„ íƒ1}}\",\n",
    "        \"2. {{ì„ íƒ2}}\",\n",
    "        \"3. ì•Œ ìˆ˜ ì—†ìŒ\",\n",
    "        \"[ì •ë‹µ]: <|eot_id|>\",\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>3<|eot_id|>\\n\",\n",
    "        \"<|start_header_id|>user<|end_header_id|>\",\n",
    "        \"[ì§ˆë¬¸]: \",\n",
    "        \"ë´‰ì‚¬ì ì˜·ì„ ê²€ì†Œí•˜ê²Œ ì…ì€ {{ì„ íƒ2}}ê³¼ ì˜·ì„ í™”ë ¤í•˜ê²Œ ì…ì€ {{ì„ íƒ1}}ì´ ë¹„í–‰ê¸°ì— íƒ”ìŠµë‹ˆë‹¤. í•œ ì‚¬ëŒì€ ë¹„í–‰ê¸° ì´ì½”ë…¸ë¯¸ì„ì—, ë‹¤ë¥¸ ì‚¬ëŒì€ í¼ìŠ¤íŠ¸í´ë˜ìŠ¤ì— ì•‰ì•„ ìˆìŠµë‹ˆë‹¤.\",\n",
    "        \"í¼ìŠ¤íŠ¸ í´ë˜ìŠ¤ì— ì•‰ì€ ì‚¬ëŒì€ ëˆ„êµ¬ì…ë‹ˆê¹Œ?\",\n",
    "        \"[ì„ íƒì§€]:\",\n",
    "        \"1. {{ì„ íƒ1}}\",\n",
    "        \"2. {{ì„ íƒ2}}\",\n",
    "        \"3. ì•Œ ìˆ˜ ì—†ìŒ\",\n",
    "        \"[ì •ë‹µ]: <|eot_id|>\",\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>3<|eot_id|>\\n\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def generate_masked_prompt(row) -> str:\n",
    "    context = row[\"context\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = ast.literal_eval(row[\"choices\"])\n",
    "\n",
    "    # Masking\n",
    "    question = question.replace(choices[0], \"{{ì„ íƒ1}}\").replace(\n",
    "        choices[1], \"{{ì„ íƒ2}}\"\n",
    "    )\n",
    "\n",
    "    prompt = \"\\n\".join(\n",
    "        [\n",
    "            SYSTEM_PROMPT,\n",
    "            MASKED_FEW_SHOT_EXAMPLES,\n",
    "            \"<|start_header_id|>user<|end_header_id|>\",\n",
    "            \"[ì§ˆë¬¸]:\",\n",
    "            context.strip(),\n",
    "            question.strip(),\n",
    "            \"[ì„ íƒì§€]:\",\n",
    "            \"1. {{ì„ íƒ1}}\",\n",
    "            \"2. {{ì„ íƒ2}}\",\n",
    "            \"3. ì•Œ ìˆ˜ ì—†ìŒ\",\n",
    "            \"[ì •ë‹µ]: <|eot_id|>\",\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        ]\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def extract_last_choice(raw_answer, choices):\n",
    "    first_digit = next((char for char in raw_answer if char.isdigit()), None)\n",
    "    if first_digit is None:\n",
    "        return \"ì•Œ ìˆ˜ ì—†ìŒ\"\n",
    "\n",
    "    if first_digit.isdigit():\n",
    "        last_choice_idx = int(first_digit)\n",
    "        if 1 <= last_choice_idx <= 3:\n",
    "            last_choice = choices[last_choice_idx - 1]\n",
    "            return last_choice\n",
    "\n",
    "    return \"ì•Œ ìˆ˜ ì—†ìŒ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdDIuBzM-bD1"
   },
   "source": [
    "## ëª¨ë¸ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq2sRkT_VCfY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPUë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc_S8d7LWOkY"
   },
   "outputs": [],
   "source": [
    "!pip install -qq accelerate bitsandbytes transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CghWJQr0-bD1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "\n",
    "def ignore_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def join_path(*args):\n",
    "    return os.path.join(BASE_DIR, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxgQClN-RucB"
   },
   "outputs": [],
   "source": [
    "# CUDA ë””ë²„ê¹…\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# CUDA ìµœì í™”\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if hasattr(torch.backends.cuda, \"matmul\") and hasattr(\n",
    "    torch.backends.cuda.matmul, \"allow_tf32\"\n",
    "):\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# ëœë¤ ì‹œë“œ ê³ ì •\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# warning ë¬´ì‹œ\n",
    "if IGNORE_WARNING:\n",
    "    ignore_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll3BdCSJ-bD3"
   },
   "outputs": [],
   "source": [
    "# Model, Tokenizer ì¤€ë¹„\n",
    "# \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_path = join_path(MODEL_DIR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "quat_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=MODEL_DEVICE_MAP,\n",
    "    quantization_config=quat_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qle6fvhkF4Yh"
   },
   "source": [
    "## ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA2tOeeUVzbb"
   },
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ ë°ì´í„° ì¤€ë¹„\n",
    "df_original = pd.read_csv(join_path(INPUT_DATA), encoding=CSV_ENCODING)\n",
    "\n",
    "# Check point í™•ì¸\n",
    "os.makedirs(join_path(CHECKPOINT_DIR), exist_ok=True)\n",
    "check_point_path = join_path(CHECKPOINT_DIR, f\"submission_{LAST_CHECK_POINT}.csv\")\n",
    "start_idx = LAST_CHECK_POINT\n",
    "\n",
    "if os.path.exists(check_point_path):\n",
    "    df_check_point = pd.read_csv(check_point_path)\n",
    "else:\n",
    "    # Check pointê°€ ì—†ì„ ë•Œ ì´ˆê¸°í™”\n",
    "    df_check_point = df_original\n",
    "    start_idx = 0\n",
    "    for col in [\"raw_input\", \"raw_output\", \"answer\"]:\n",
    "        if col not in df_check_point.columns:\n",
    "            df_check_point[col] = \"\"\n",
    "        df_check_point[col] = df_check_point[col].astype(\"string\")\n",
    "\n",
    "total_data_size = len(df_check_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qisa9v8XHkp"
   },
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ëŠ” ë¯¸ë¦¬ ì „ì²˜ë¦¬\n",
    "user_init_prompts = [None] * total_data_size\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {\n",
    "        executor.submit(generate_masked_prompt, row): idx\n",
    "        for idx, row in df_original.iterrows()\n",
    "    }\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        idx = futures[future]\n",
    "        user_init_prompts[idx] = future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7etKPdxWXHJ3"
   },
   "source": [
    "## ë‹µë³€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i477X_IPZlT6"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tokenize_batch(batch_prompts):\n",
    "    return tokenizer(\n",
    "        batch_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZER_MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_batch(batch_tokens, max_new_tokens):\n",
    "    answer_tokens = model.generate(\n",
    "        input_ids=batch_tokens[\"input_ids\"],\n",
    "        attention_mask=batch_tokens[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_k=TOP_K,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    decoded_answers = tokenizer.batch_decode(\n",
    "        answer_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS\n",
    "    )\n",
    "    return decoded_answers\n",
    "\n",
    "\n",
    "def split_answer(answer) -> tuple[str, str]:\n",
    "    prompt, raw_answer = answer.rsplit(\"assistant\", 1)\n",
    "    return prompt, raw_answer\n",
    "\n",
    "\n",
    "def pipeline(batch_prompts) -> list[str]:\n",
    "    question_tokens = tokenize_batch(batch_prompts)\n",
    "    answer_tokens = process_batch(question_tokens, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    return answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPBDil_LPYlv"
   },
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ë° cuda cache ì •ë¦¬\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# ëª¨ë¸ ì¶”ë¡  ì‹œì‘\n",
    "start_time = time.time()\n",
    "while start_idx < total_data_size:\n",
    "    end_idx = min(start_idx + BATCH_SIZE, total_data_size)\n",
    "    batch_prompts = user_init_prompts[start_idx:end_idx]\n",
    "    batch_answers = pipeline(batch_prompts)\n",
    "\n",
    "    for idx, answer in enumerate(batch_answers):\n",
    "        idx = idx + start_idx\n",
    "        prompt, raw_answer = split_answer(answer)\n",
    "        df_check_point.at[idx, \"raw_input\"] = prompt\n",
    "        df_check_point.at[idx, \"raw_output\"] = raw_answer\n",
    "        choices = ast.literal_eval(df_original.at[idx, \"choices\"])\n",
    "        df_check_point.at[idx, \"answer\"] = extract_last_choice(raw_answer, choices)\n",
    "\n",
    "        if idx % CHECK_POINT_STEP == 0:\n",
    "            # Check pointì—ì„œ ë‹µë³€ì„ íŒŒì¼ë¡œ ì €ì¥\n",
    "            end_time = time.time()\n",
    "            df_check_point[[\"ID\", \"raw_input\", \"raw_output\", \"answer\"]].to_csv(\n",
    "                join_path(CHECKPOINT_DIR, f\"submission_{str(idx)}.csv\"),\n",
    "                index=False,\n",
    "                encoding=CSV_ENCODING,\n",
    "            )\n",
    "            print(\n",
    "                f\"âœ…{idx}/{total_data_size} ì €ì¥. ({(end_time - start_time) / 60:.1f}ë¶„)\"\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    start_idx = end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o2Ku-M8RucD"
   },
   "source": [
    "## ì œì¶œ íŒŒì¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVGOi4l2PYlv"
   },
   "outputs": [],
   "source": [
    "# ìµœì¢… íŒŒì¼ ì €ì¥\n",
    "submission = df_check_point[[\"ID\", \"raw_input\", \"raw_output\", \"answer\"]]\n",
    "submission.to_csv(join_path(OUTPUT_FILE), index=False, encoding=CSV_ENCODING)\n",
    "print(\"ğŸ« ê¸°ë¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
